{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Brax Training with PyTorch on GPU",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trVNqxHmGISS"
      },
      "source": [
        "# Training in Brax with PyTorch on GPUs\n",
        "\n",
        "Brax is ready to integrate into other research toolkits by way of the [OpenAI Gym](https://gym.openai.com/) interface.  Brax environments convert to Gym environments using either [GymWrapper](https://github.com/google/brax/blob/main/brax/envs/wrappers.py) for single environments, or [VectorGymWrapper](https://github.com/google/brax/blob/main/brax/envs/wrappers.py) for batched (parallelized) environments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJhPpM5ZPrpq"
      },
      "source": [
        "#@title Import Brax and some helper modules\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import collections\n",
        "from datetime import datetime\n",
        "import functools\n",
        "import math\n",
        "import time\n",
        "from typing import Any, Callable, Dict, Optional, Sequence\n",
        "\n",
        "try:\n",
        "  import brax\n",
        "except ImportError:\n",
        "  !pip install git+https://github.com/google/brax.git@main\n",
        "  clear_output()\n",
        "  import brax\n",
        "\n",
        "from brax import envs\n",
        "from brax.envs import to_torch\n",
        "from brax.io import metrics\n",
        "from brax.training.agents.ppo import train as ppo\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# have torch allocate on device first, to prevent JAX from swallowing up all the\n",
        "# GPU memory. By default JAX will pre-allocate 90% of the available GPU memory:\n",
        "# https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html\n",
        "v = torch.ones(1, device='cuda')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQFCkfu8Qwre"
      },
      "source": [
        "Here is a PPO Agent written in PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWJE4b5BHeH7"
      },
      "source": [
        "class Agent(nn.Module):\n",
        "  \"\"\"Standard PPO Agent with GAE and observation normalization.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               policy_layers: Sequence[int],\n",
        "               value_layers: Sequence[int],\n",
        "               entropy_cost: float,\n",
        "               discounting: float,\n",
        "               reward_scaling: float,\n",
        "               device: str):\n",
        "    super(Agent, self).__init__()\n",
        "\n",
        "    policy = []\n",
        "    for w1, w2 in zip(policy_layers, policy_layers[1:]):\n",
        "      policy.append(nn.Linear(w1, w2))\n",
        "      policy.append(nn.SiLU())\n",
        "    policy.pop()  # drop the final activation\n",
        "    self.policy = nn.Sequential(*policy)\n",
        "\n",
        "    value = []\n",
        "    for w1, w2 in zip(value_layers, value_layers[1:]):\n",
        "      value.append(nn.Linear(w1, w2))\n",
        "      value.append(nn.SiLU())\n",
        "    value.pop()  # drop the final activation\n",
        "    self.value = nn.Sequential(*value)\n",
        "\n",
        "    self.num_steps = torch.zeros((), device=device)\n",
        "    self.running_mean = torch.zeros(policy_layers[0], device=device)\n",
        "    self.running_variance = torch.zeros(policy_layers[0], device=device)\n",
        "\n",
        "    self.entropy_cost = entropy_cost\n",
        "    self.discounting = discounting\n",
        "    self.reward_scaling = reward_scaling\n",
        "    self.lambda_ = 0.95\n",
        "    self.epsilon = 0.3\n",
        "    self.device = device\n",
        "\n",
        "  @torch.jit.export\n",
        "  def dist_create(self, logits):\n",
        "    \"\"\"Normal followed by tanh.\n",
        "\n",
        "    torch.distribution doesn't work with torch.jit, so we roll our own.\"\"\"\n",
        "    loc, scale = torch.split(logits, logits.shape[-1] // 2, dim=-1)\n",
        "    scale = F.softplus(scale) + .001\n",
        "    return loc, scale\n",
        "\n",
        "  @torch.jit.export\n",
        "  def dist_sample_no_postprocess(self, loc, scale):\n",
        "    return torch.normal(loc, scale)\n",
        "\n",
        "  @classmethod\n",
        "  def dist_postprocess(cls, x):\n",
        "    return torch.tanh(x)\n",
        "\n",
        "  @torch.jit.export\n",
        "  def dist_entropy(self, loc, scale):\n",
        "    log_normalized = 0.5 * math.log(2 * math.pi) + torch.log(scale)\n",
        "    entropy = 0.5 + log_normalized\n",
        "    entropy = entropy * torch.ones_like(loc)\n",
        "    dist = torch.normal(loc, scale)\n",
        "    log_det_jacobian = 2 * (math.log(2) - dist - F.softplus(-2 * dist))\n",
        "    entropy = entropy + log_det_jacobian\n",
        "    return entropy.sum(dim=-1)\n",
        "\n",
        "  @torch.jit.export\n",
        "  def dist_log_prob(self, loc, scale, dist):\n",
        "    log_unnormalized = -0.5 * ((dist - loc) / scale).square()\n",
        "    log_normalized = 0.5 * math.log(2 * math.pi) + torch.log(scale)\n",
        "    log_det_jacobian = 2 * (math.log(2) - dist - F.softplus(-2 * dist))\n",
        "    log_prob = log_unnormalized - log_normalized - log_det_jacobian\n",
        "    return log_prob.sum(dim=-1)\n",
        "\n",
        "  @torch.jit.export\n",
        "  def update_normalization(self, observation):\n",
        "    self.num_steps += observation.shape[0] * observation.shape[1]\n",
        "    input_to_old_mean = observation - self.running_mean\n",
        "    mean_diff = torch.sum(input_to_old_mean / self.num_steps, dim=(0, 1))\n",
        "    self.running_mean = self.running_mean + mean_diff\n",
        "    input_to_new_mean = observation - self.running_mean\n",
        "    var_diff = torch.sum(input_to_new_mean * input_to_old_mean, dim=(0, 1))\n",
        "    self.running_variance = self.running_variance + var_diff\n",
        "\n",
        "  @torch.jit.export\n",
        "  def normalize(self, observation):\n",
        "    variance = self.running_variance / (self.num_steps + 1.0)\n",
        "    variance = torch.clip(variance, 1e-6, 1e6)\n",
        "    return ((observation - self.running_mean) / variance.sqrt()).clip(-5, 5)\n",
        "\n",
        "  @torch.jit.export\n",
        "  def get_logits_action(self, observation):\n",
        "    observation = self.normalize(observation)\n",
        "    logits = self.policy(observation)\n",
        "    loc, scale = self.dist_create(logits)\n",
        "    action = self.dist_sample_no_postprocess(loc, scale)\n",
        "    return logits, action\n",
        "\n",
        "  @torch.jit.export\n",
        "  def compute_gae(self, truncation, termination, reward, values,\n",
        "                  bootstrap_value):\n",
        "    truncation_mask = 1 - truncation\n",
        "    # Append bootstrapped value to get [v1, ..., v_t+1]\n",
        "    values_t_plus_1 = torch.cat(\n",
        "        [values[1:], torch.unsqueeze(bootstrap_value, 0)], dim=0)\n",
        "    deltas = reward + self.discounting * (\n",
        "        1 - termination) * values_t_plus_1 - values\n",
        "    deltas *= truncation_mask\n",
        "\n",
        "    acc = torch.zeros_like(bootstrap_value)\n",
        "    vs_minus_v_xs = torch.zeros_like(truncation_mask)\n",
        "\n",
        "    for ti in range(truncation_mask.shape[0]):\n",
        "      ti = truncation_mask.shape[0] - ti - 1\n",
        "      acc = deltas[ti] + self.discounting * (\n",
        "          1 - termination[ti]) * truncation_mask[ti] * self.lambda_ * acc\n",
        "      vs_minus_v_xs[ti] = acc\n",
        "\n",
        "    # Add V(x_s) to get v_s.\n",
        "    vs = vs_minus_v_xs + values\n",
        "    vs_t_plus_1 = torch.cat([vs[1:], torch.unsqueeze(bootstrap_value, 0)], 0)\n",
        "    advantages = (reward + self.discounting *\n",
        "                  (1 - termination) * vs_t_plus_1 - values) * truncation_mask\n",
        "    return vs, advantages\n",
        "\n",
        "  @torch.jit.export\n",
        "  def loss(self, td: Dict[str, torch.Tensor]):\n",
        "    observation = self.normalize(td['observation'])\n",
        "    policy_logits = self.policy(observation[:-1])\n",
        "    baseline = self.value(observation)\n",
        "    baseline = torch.squeeze(baseline, dim=-1)\n",
        "\n",
        "    # Use last baseline value (from the value function) to bootstrap.\n",
        "    bootstrap_value = baseline[-1]\n",
        "    baseline = baseline[:-1]\n",
        "    reward = td['reward'] * self.reward_scaling\n",
        "    termination = td['done'] * (1 - td['truncation'])\n",
        "\n",
        "    loc, scale = self.dist_create(td['logits'])\n",
        "    behaviour_action_log_probs = self.dist_log_prob(loc, scale, td['action'])\n",
        "    loc, scale = self.dist_create(policy_logits)\n",
        "    target_action_log_probs = self.dist_log_prob(loc, scale, td['action'])\n",
        "\n",
        "    with torch.no_grad():\n",
        "      vs, advantages = self.compute_gae(\n",
        "          truncation=td['truncation'],\n",
        "          termination=termination,\n",
        "          reward=reward,\n",
        "          values=baseline,\n",
        "          bootstrap_value=bootstrap_value)\n",
        "\n",
        "    rho_s = torch.exp(target_action_log_probs - behaviour_action_log_probs)\n",
        "    surrogate_loss1 = rho_s * advantages\n",
        "    surrogate_loss2 = rho_s.clip(1 - self.epsilon,\n",
        "                                 1 + self.epsilon) * advantages\n",
        "    policy_loss = -torch.mean(torch.minimum(surrogate_loss1, surrogate_loss2))\n",
        "\n",
        "    # Value function loss\n",
        "    v_error = vs - baseline\n",
        "    v_loss = torch.mean(v_error * v_error) * 0.5 * 0.5\n",
        "\n",
        "    # Entropy reward\n",
        "    entropy = torch.mean(self.dist_entropy(loc, scale))\n",
        "    entropy_loss = self.entropy_cost * -entropy\n",
        "\n",
        "    return policy_loss + v_loss + entropy_loss"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWbuk7IAR0SU"
      },
      "source": [
        "Finally, some code for unrolling and batching environment data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3y5o7-oSBm-"
      },
      "source": [
        "StepData = collections.namedtuple(\n",
        "    'StepData',\n",
        "    ('observation', 'logits', 'action', 'reward', 'done', 'truncation'))\n",
        "\n",
        "\n",
        "def sd_map(f: Callable[..., torch.Tensor], *sds) -> StepData:\n",
        "  \"\"\"Map a function over each field in StepData.\"\"\"\n",
        "  items = {}\n",
        "  keys = sds[0]._asdict().keys()\n",
        "  for k in keys:\n",
        "    items[k] = f(*[sd._asdict()[k] for sd in sds])\n",
        "  return StepData(**items)\n",
        "\n",
        "\n",
        "def eval_unroll(agent, env, length):\n",
        "  \"\"\"Return number of episodes and average reward for a single unroll.\"\"\"\n",
        "  observation = env.reset()\n",
        "  episodes = torch.zeros((), device=agent.device)\n",
        "  episode_reward = torch.zeros((), device=agent.device)\n",
        "  for _ in range(length):\n",
        "    _, action = agent.get_logits_action(observation)\n",
        "    observation, reward, done, _ = env.step(Agent.dist_postprocess(action))\n",
        "    episodes += torch.sum(done)\n",
        "    episode_reward += torch.sum(reward)\n",
        "  return episodes, episode_reward / episodes\n",
        "\n",
        "\n",
        "def train_unroll(agent, env, observation, num_unrolls, unroll_length):\n",
        "  \"\"\"Return step data over multple unrolls.\"\"\"\n",
        "  sd = StepData([], [], [], [], [], [])\n",
        "  for _ in range(num_unrolls):\n",
        "    one_unroll = StepData([observation], [], [], [], [], [])\n",
        "    for _ in range(unroll_length):\n",
        "      logits, action = agent.get_logits_action(observation)\n",
        "      observation, reward, done, info = env.step(Agent.dist_postprocess(action))\n",
        "      one_unroll.observation.append(observation)\n",
        "      one_unroll.logits.append(logits)\n",
        "      one_unroll.action.append(action)\n",
        "      one_unroll.reward.append(reward)\n",
        "      one_unroll.done.append(done)\n",
        "      one_unroll.truncation.append(info['truncation'])\n",
        "    one_unroll = sd_map(torch.stack, one_unroll)\n",
        "    sd = sd_map(lambda x, y: x + [y], sd, one_unroll)\n",
        "  td = sd_map(torch.stack, sd)\n",
        "  return observation, td\n",
        "\n",
        "\n",
        "def train(\n",
        "    env_name: str = 'ant',\n",
        "    num_envs: int = 2048,\n",
        "    episode_length: int = 1000,\n",
        "    device: str = 'cuda',\n",
        "    num_timesteps: int = 30_000_000,\n",
        "    eval_frequency: int = 10,\n",
        "    unroll_length: int = 5,\n",
        "    batch_size: int = 1024,\n",
        "    num_minibatches: int = 32,\n",
        "    num_update_epochs: int = 4,\n",
        "    reward_scaling: float = .1,\n",
        "    entropy_cost: float = 1e-2,\n",
        "    discounting: float = .97,\n",
        "    learning_rate: float = 3e-4,\n",
        "    progress_fn: Optional[Callable[[int, Dict[str, Any]], None]] = None,\n",
        "):\n",
        "  \"\"\"Trains a policy via PPO.\"\"\"\n",
        "  gym_name = f'brax-{env_name}-v0'\n",
        "  if gym_name not in gym.envs.registry.env_specs:\n",
        "    entry_point = functools.partial(envs.create_gym_env, env_name=env_name)\n",
        "    gym.register(gym_name, entry_point=entry_point)\n",
        "  env = gym.make(gym_name, batch_size=num_envs, episode_length=episode_length)\n",
        "  # automatically convert between jax ndarrays and torch tensors:\n",
        "  env = to_torch.JaxToTorchWrapper(env, device=device)\n",
        "\n",
        "  # env warmup\n",
        "  env.reset()\n",
        "  action = torch.zeros(env.action_space.shape).to(device)\n",
        "  env.step(action)\n",
        "\n",
        "  # create the agent\n",
        "  policy_layers = [\n",
        "      env.observation_space.shape[-1], 64, 64, env.action_space.shape[-1] * 2\n",
        "  ]\n",
        "  value_layers = [env.observation_space.shape[-1], 64, 64, 1]\n",
        "  agent = Agent(policy_layers, value_layers, entropy_cost, discounting,\n",
        "                reward_scaling, device)\n",
        "  agent = torch.jit.script(agent.to(device))\n",
        "  optimizer = optim.Adam(agent.parameters(), lr=learning_rate)\n",
        "\n",
        "  sps = 0\n",
        "  total_steps = 0\n",
        "  total_loss = 0\n",
        "  for eval_i in range(eval_frequency + 1):\n",
        "    if progress_fn:\n",
        "      t = time.time()\n",
        "      with torch.no_grad():\n",
        "        episode_count, episode_reward = eval_unroll(agent, env, episode_length)\n",
        "      duration = time.time() - t\n",
        "      # TODO: only count stats from completed episodes\n",
        "      episode_avg_length = env.num_envs * episode_length / episode_count\n",
        "      eval_sps = env.num_envs * episode_length / duration\n",
        "      progress = {\n",
        "          'eval/episode_reward': episode_reward,\n",
        "          'eval/completed_episodes': episode_count,\n",
        "          'eval/avg_episode_length': episode_avg_length,\n",
        "          'speed/sps': sps,\n",
        "          'speed/eval_sps': eval_sps,\n",
        "          'losses/total_loss': total_loss,\n",
        "      }\n",
        "      progress_fn(total_steps, progress)\n",
        "\n",
        "    if eval_i == eval_frequency:\n",
        "      break\n",
        "\n",
        "    observation = env.reset()\n",
        "    num_steps = batch_size * num_minibatches * unroll_length\n",
        "    num_epochs = num_timesteps // (num_steps * eval_frequency)\n",
        "    num_unrolls = batch_size * num_minibatches // env.num_envs\n",
        "    total_loss = 0\n",
        "    t = time.time()\n",
        "    for _ in range(num_epochs):\n",
        "      observation, td = train_unroll(agent, env, observation, num_unrolls,\n",
        "                                     unroll_length)\n",
        "\n",
        "      # make unroll first\n",
        "      def unroll_first(data):\n",
        "        data = data.swapaxes(0, 1)\n",
        "        return data.reshape([data.shape[0], -1] + list(data.shape[3:]))\n",
        "      td = sd_map(unroll_first, td)\n",
        "\n",
        "      # update normalization statistics\n",
        "      agent.update_normalization(td.observation)\n",
        "\n",
        "      for _ in range(num_update_epochs):\n",
        "        # shuffle and batch the data\n",
        "        with torch.no_grad():\n",
        "          permutation = torch.randperm(td.observation.shape[1], device=device)\n",
        "          def shuffle_batch(data):\n",
        "            data = data[:, permutation]\n",
        "            data = data.reshape([data.shape[0], num_minibatches, -1] +\n",
        "                                list(data.shape[2:]))\n",
        "            return data.swapaxes(0, 1)\n",
        "          epoch_td = sd_map(shuffle_batch, td)\n",
        "\n",
        "        for minibatch_i in range(num_minibatches):\n",
        "          td_minibatch = sd_map(lambda d: d[minibatch_i], epoch_td)\n",
        "          loss = agent.loss(td_minibatch._asdict())\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          total_loss += loss.detach()\n",
        "\n",
        "    duration = time.time() - t\n",
        "    total_steps += num_epochs * num_steps\n",
        "    total_loss = total_loss / (num_epochs * num_update_epochs * num_minibatches)\n",
        "    sps = num_epochs * num_steps / duration"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2A9MMlHUajH"
      },
      "source": [
        "Let's go!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-lrKHvkUeYM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "447a410f-39dd-4f2e-aadf-927098226bcd"
      },
      "source": [
        "xdata = []\n",
        "ydata = []\n",
        "eval_sps = []\n",
        "train_sps = []\n",
        "times = [datetime.now()]\n",
        "\n",
        "def progress(num_steps, metrics):\n",
        "  times.append(datetime.now())\n",
        "  xdata.append(num_steps)\n",
        "  # copy to cpu, otherwise matplotlib throws an exception\n",
        "  reward = metrics['eval/episode_reward'].cpu()\n",
        "  ydata.append(reward)\n",
        "  eval_sps.append(metrics['speed/eval_sps'])\n",
        "  train_sps.append(metrics['speed/sps'])\n",
        "  clear_output(wait=True)\n",
        "  plt.xlim([0, 30_000_000])\n",
        "  plt.ylim([0, 6000])\n",
        "  plt.xlabel('# environment steps')\n",
        "  plt.ylabel('reward per episode')\n",
        "  plt.plot(xdata, ydata)\n",
        "  plt.show()\n",
        "\n",
        "train(progress_fn=progress)\n",
        "\n",
        "print(f'time to jit: {times[1] - times[0]}')\n",
        "print(f'time to train: {times[-1] - times[1]}')\n",
        "print(f'eval steps/sec: {np.mean(eval_sps[1:])}')\n",
        "print(f'train steps/sec: {np.mean(train_sps[1:])}')\n",
        "!nvidia-smi -L"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time to jit: 0:00:29.043245\n",
            "time to train: 0:03:45.831491\n",
            "eval steps/sec: 459100.05817018624\n",
            "train steps/sec: 165690.40442955276\n",
            "GPU 0: A100-SXM4-40GB (UUID: GPU-3fa97613-4b5c-cd4d-0aee-c553af66dba5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2p-20bCi4iI"
      },
      "source": [
        "In this arrangement, we can rollout environment steps much faster than we can train: the speed at which PyTorch can backpropagate the loss and step the optimizer is the bottleneck.  This PyTorch code can probably be sped up by adding [automatic mixed precision](https://pytorch.org/docs/stable/notes/amp_examples.html), and following other recommendations in the [PyTorch performance tuning guide](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html).\n",
        "\n",
        "We know we have a fair bit of headroom to improve the PyTorch implementation, as the built-in Brax trainer (which uses [flax.optim](https://flax.readthedocs.io/en/latest/flax.optim.html)) runs at nearly double the steps per second:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xmuz3I21p35H",
        "outputId": "a1b2fbb4-01dd-441a-ff73-cd9be69e0187"
      },
      "source": [
        "train_sps = []\n",
        "\n",
        "def progress(_, metrics):\n",
        "  if 'training/sps' in metrics:\n",
        "    train_sps.append(metrics['training/sps'])\n",
        "\n",
        "ppo.train(\n",
        "    environment=envs.get_environment(env_name='ant'), num_timesteps = 30_000_000,\n",
        "    num_evals = 10, reward_scaling = .1, episode_length = 1000,\n",
        "    normalize_observations = True, action_repeat = 1, unroll_length = 5,\n",
        "    num_minibatches = 32, num_updates_per_batch = 4, discounting = 0.97,\n",
        "    learning_rate = 3e-4, entropy_cost = 1e-2, num_envs = 2048,\n",
        "    batch_size = 1024, progress_fn = progress)\n",
        "\n",
        "print(f'train steps/sec: {np.mean(train_sps[1:])}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train steps/sec: 426827.7156297093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqXKdDwVL6L4"
      },
      "source": [
        "tunaalabagana! ðŸ‘‹"
      ]
    }
  ]
}