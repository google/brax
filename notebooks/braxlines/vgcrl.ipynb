{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Brax VGCRL.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssCOanHc8JH_"
      },
      "source": [
        "# Training Goal-Conditioned and Unsupervised RL Agents in Brax\n",
        "\n",
        "In [Brax Training](https://colab.research.google.com/github/google/brax/blob/main/notebooks/training.ipynb) we tried out [gym](https://gym.openai.com/)-like environments and PPO, SAC, evolutionary search, and trajectory optimization algorithms. We can build various RL algorithms on top of these ultra-fast implementations. This colab runs a family of [variational GCRL](https://arxiv.org/abs/2106.01404) algorithms, which includes [goal-conditioned RL](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.51.3077) and [DIAYN](https://arxiv.org/abs/1802.06070) as special cases. Let's try it out!\n",
        "\n",
        "This provides a bare bone implementation based on minimal modifications to the\n",
        "baseline [PPO](https://github.com/google/brax/blob/main/brax/training/ppo.py),\n",
        "enabling training in a few minutes. More features, tunings, and benchmarked results will be added soon."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sOmCoOrF0F8"
      },
      "source": [
        "#@title Colab setup and imports\n",
        "#@markdown ## ⚠️ PLEASE NOTE:\n",
        "#@markdown This colab runs best using a TPU runtime.  From the Colab menu, choose Runtime > Change Runtime Type, then select **'TPU'** in the dropdown.\n",
        "\n",
        "from datetime import datetime\n",
        "import functools\n",
        "import os\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.tools import colab_tpu\n",
        "from IPython.display import HTML, clear_output \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "try:\n",
        "  import brax\n",
        "except ImportError:\n",
        "  !pip install git+https://github.com/google/brax.git@main\n",
        "  clear_output()\n",
        "  import brax\n",
        "\n",
        "from brax import envs\n",
        "from brax.io import html\n",
        "from brax.experimental.braxlines.training import ppo\n",
        "from brax.experimental.braxlines.vgcrl import utils as vgcrl_utils\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  from jax.tools import colab_tpu\n",
        "  colab_tpu.setup_tpu()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaJDZqhCLovU"
      },
      "source": [
        "#@title Visualizing pre-included Brax environments { run: \"auto\" }\n",
        "#@markdown Note: Similarly to how experiments were run in\n",
        "#@markdown [DIAYN](https://arxiv.org/abs/1802.06070) \n",
        "#@markdown and [variational GCRL](https://arxiv.org/abs/2106.01404),\n",
        "#@markdown we assume some prior knowledge about interesting dimensions\n",
        "#@markdown of the environment `env_indices` (except `exp_name`='diayn_full').\n",
        "#@markdown This is also used for skill visualization later.\n",
        "\n",
        "env_name = \"ant\"  # @param ['ant', 'halfcheetah']\n",
        "exp_name = \"diayn\"  # @param ['gcrl', 'cdiayn', 'diayn', 'diayn_full']\n",
        "diayn_num_skills =   8# @param {type: 'integer'}\n",
        "logits_clip_range = 10.0# @param {'type': 'number'} \n",
        "env_indices = {\n",
        "    'ant': (13, 14),  # x-y velocities\n",
        "    'humanoid': (22, 23),  # x-y velocities\n",
        "    'halfcheetah': (11,),  # x velocity\n",
        "}[env_name]\n",
        "base_env_fn = envs.create_fn(env_name=env_name)\n",
        "base_env = base_env_fn()\n",
        "env_obs_size = base_env.observation_size\n",
        "\n",
        "disc_fn = {\n",
        "    'gcrl': functools.partial(\n",
        "        vgcrl_utils.Discriminator,\n",
        "        q_fn='indexing',\n",
        "        z_size=len(env_indices), \n",
        "        q_fn_params=dict(indices=env_indices),\n",
        "        dist_p_params = dict(scale=2.),\n",
        "        dist_q_params = dict(scale=2.),\n",
        "        ),\n",
        "    'cdiayn': functools.partial(\n",
        "        vgcrl_utils.Discriminator,\n",
        "        q_fn='indexing_mlp', \n",
        "        z_size=len(env_indices), \n",
        "        fn_params=dict(\n",
        "            indices = env_indices,\n",
        "            output_size = len(env_indices),\n",
        "            ),\n",
        "        ),\n",
        "    'diayn': functools.partial(\n",
        "        vgcrl_utils.Discriminator,\n",
        "        q_fn='indexing_mlp', \n",
        "        z_size=diayn_num_skills,\n",
        "        q_fn_params=dict(\n",
        "            indices = env_indices,\n",
        "            output_size=diayn_num_skills,\n",
        "            ),\n",
        "        dist_p = 'UniformCategorial',\n",
        "        dist_q = 'Categorial',\n",
        "        logits_clip_range=logits_clip_range,\n",
        "        ),\n",
        "    'diayn_full': functools.partial(\n",
        "        vgcrl_utils.Discriminator,\n",
        "        q_fn='mlp', \n",
        "        z_size=diayn_num_skills,\n",
        "        q_fn_params=dict(\n",
        "            input_size = env_obs_size,\n",
        "            output_size=diayn_num_skills,\n",
        "            ),\n",
        "        dist_p = 'UniformCategorial',\n",
        "        dist_q = 'Categorial',\n",
        "        logits_clip_range=logits_clip_range,\n",
        "        ),\n",
        "}[exp_name]\n",
        "disc = disc_fn()\n",
        "extra_params = disc.init_model(rng=jax.random.PRNGKey(seed=0))\n",
        "env_fn = vgcrl_utils.create_fn(env_name=env_name, disc=disc)\n",
        "env = env_fn()\n",
        "\n",
        "def visualize(sys, qps, save_path: str = None):\n",
        "  \"\"\"Renders a 3D visualization of the environment.\"\"\"\n",
        "  if save_path:\n",
        "    html.save_html(save_path, sys, qps)\n",
        "  return HTML(html.render(sys, qps))\n",
        "\n",
        "jit_env_reset = jax.jit(env.reset)\n",
        "state = jit_env_reset(rng=jax.random.PRNGKey(seed=0))\n",
        "clear_output()  # clear out jax.lax warning before rendering\n",
        "\n",
        "# Visualize in 3D\n",
        "visualize(env.sys, [state.qp])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vgMSWODfyMC"
      },
      "source": [
        "#@title Training some pre-included Brax environments\n",
        "num_timesteps_multiplier =  2# @param {type: 'integer'}\n",
        "disc_update_ratio = 1.0# @param {'type': 'number'}\n",
        "\n",
        "# We determined some reasonable hyperparameters offline and share them here.\n",
        "n = num_timesteps_multiplier\n",
        "train_fn = {\n",
        "  'ant': functools.partial(\n",
        "      ppo.train, num_timesteps = 50_000_000*n, log_frequency = 20,\n",
        "      reward_scaling = 10, episode_length = 1000, normalize_observations = True,\n",
        "      action_repeat = 1, unroll_length = 5, num_minibatches = 32,\n",
        "      num_update_epochs = 4, discounting = 0.95, learning_rate = 3e-4,\n",
        "      entropy_cost = 1e-2, num_envs = 2048, batch_size = 1024\n",
        "  ),\n",
        "  'halfcheetah': functools.partial(\n",
        "      ppo.train, num_timesteps = 50_000_000*n, log_frequency = 10,\n",
        "      reward_scaling = 1, episode_length = 1000, normalize_observations = True,\n",
        "      action_repeat = 1, unroll_length = 20, num_minibatches = 32,\n",
        "      num_update_epochs = 8, discounting = 0.95, learning_rate = 3e-4,\n",
        "      entropy_cost = 0.001, num_envs = 2048, batch_size = 512\n",
        "  ),\n",
        "}[env_name]\n",
        "\n",
        "times = [datetime.now()]\n",
        "plotdata = {}\n",
        "plotkeys = [\n",
        "  'eval/episode_reward',\n",
        "  'losses/disc_loss',\n",
        "]\n",
        "\n",
        "def progress(num_steps, metrics, _):\n",
        "  times.append(datetime.now())\n",
        "  for key, v in metrics.items():\n",
        "    plotdata[key] = plotdata.get(key, dict(x=[], y=[]))\n",
        "    plotdata[key]['x'] += [num_steps]\n",
        "    plotdata[key]['y'] += [v]\n",
        "  clear_output(wait=True)\n",
        "  num_figs = len(plotkeys)\n",
        "  fig, axs = plt.subplots(ncols=num_figs, figsize=(3.5*num_figs, 3))\n",
        "  for i, key in enumerate(plotkeys):\n",
        "    if key in plotdata:\n",
        "      axs[i].plot(plotdata[key]['x'], plotdata[key]['y'])\n",
        "    axs[i].set(xlabel='# environment steps', ylabel=key)\n",
        "    axs[i].set_xlim([0, train_fn.keywords['num_timesteps']])\n",
        "  fig.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "extra_loss_fns = dict(disc_loss=functools.partial(\n",
        "    vgcrl_utils.disc_loss_fn, \n",
        "    disc=disc)) if extra_params else None\n",
        "extra_loss_update_ratios = dict(disc_loss=disc_update_ratio\n",
        "                                ) if extra_params else None\n",
        "inference_fn, params, _ = train_fn(environment_fn=env_fn, \n",
        "                                   progress_fn=progress,\n",
        "                                   extra_params=extra_params,\n",
        "                                   extra_loss_fns=extra_loss_fns,\n",
        "                                   extra_loss_update_ratios=extra_loss_update_ratios,\n",
        "                                   )\n",
        "\n",
        "print(f'time to jit: {times[1] - times[0]}')\n",
        "print(f'time to train: {times[-1] - times[1]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNMLEyaTspEM"
      },
      "source": [
        "#@title Visualizing a trajectory of the learned inference function\n",
        "rollout_z = \"fix\"  # @param ['sample', 'fix']\n",
        "diayn_index =   7# @param {type: 'integer'}\n",
        "seed = 0  # @param {type: 'integer'}\n",
        "save_path = '/tmp/{env_name}_{exp_name}_{index}.html' # @param {'type': 'raw'}\n",
        "save_path = save_path.format(env_name=env_name, exp_name=exp_name, index=diayn_index) \n",
        "\n",
        "jit_env_step = jax.jit(env.step)\n",
        "jit_inference_fn = jax.jit(inference_fn)\n",
        "qps = []\n",
        "states = []\n",
        "z = {\n",
        "    'gcrl':jnp.ones(env.z_size) * 2.,\n",
        "    'cdiayn':jnp.ones(env.z_size),\n",
        "    'diayn': jax.nn.one_hot(jnp.array(diayn_index), env.z_size),\n",
        "    'diayn_full': jax.nn.one_hot(jnp.array(diayn_index), env.z_size),\n",
        "         }[exp_name] if rollout_z == \"fix\" else None\n",
        "state = jit_env_reset(rng=jax.random.PRNGKey(seed=seed), z=z)\n",
        "while not state.done:\n",
        "  qps.append(state.qp)\n",
        "  states.append(state)\n",
        "  act = jit_inference_fn(params, state.obs, state.rng)\n",
        "  state = jit_env_step(state, act, params[-1])\n",
        "\n",
        "visualize(env.sys, qps, save_path=save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5eWOxg7RmQQ"
      },
      "source": [
        "#@title Visualizing skills of the learned inference function in 2D plot\n",
        "import numpy as np\n",
        "from itertools import product\n",
        "num_samples_per_z = 5  # @param {type: 'integer'}\n",
        "time_subsampling = 10  # @param {type: 'integer'}\n",
        "time_last_n = 500 # @param {type: 'integer'}\n",
        "seed = 0  # @param {type: 'integer'}\n",
        "\n",
        "O = env_obs_size\n",
        "Z = env.z_size\n",
        "M = num_samples_per_z\n",
        "\n",
        "# Sample {D} z's\n",
        "if exp_name in ('gcrl', 'vgcrl'):\n",
        "  batch_z = jnp.array(list(product(*([[-1,1]] * env.param_size))))\n",
        "elif exp_name in ('diayn', 'diayn_full'):\n",
        "  batch_z = jax.nn.one_hot(jnp.arange(0, Z), Z)\n",
        "D = batch_z.shape[0]\n",
        "\n",
        "# Repeat each z by {M} times\n",
        "batch_z = jnp.repeat(batch_z, M, axis=0) # [D*M, Z] \n",
        "\n",
        "# Reset and run environment\n",
        "batch_env = env_fn(batch_size=D*M)\n",
        "state = batch_env.reset(\n",
        "    jnp.array([jax.random.PRNGKey(seed+i) for i in range(D*M)]),\n",
        "    z=batch_z)\n",
        "states = [state]\n",
        "jit_step = jax.jit(batch_env.step)\n",
        "jit_inference_fn = jax.jit(inference_fn)\n",
        "while not state.done.all():\n",
        "  act = jit_inference_fn(params, state.obs, state.rng[0])\n",
        "  state = jit_step(state, act, params[-1])\n",
        "  states.append(state)\n",
        "\n",
        "# Get indices of interest\n",
        "obses = jnp.stack([state.obs for state in states],\n",
        "                  axis=0)[-time_last_n:][::time_subsampling] # [T, D*M, O+D]\n",
        "print(f'T={obses.shape[0]}, O={O}, Z={Z}, D={D}, M={M}')\n",
        "env_obses, _ = batch_env.disc.split_obs(obses) # [T, D*M, O]\n",
        "env_vars = env_obses[..., env_indices] # [T, D*M, 1 or 2]\n",
        "if env_vars.shape[-1] == 1:\n",
        "  env_vars = jnp.concatenate([env_vars, jnp.zeros(env_vars.shape)], axis=-1)\n",
        "assert env_vars.shape[1:] == (D*M, 2), f'{env_vars.shape} incompatible {(D*M,2)}'\n",
        "env_vars = env_vars.reshape(-1, D, M, 2).swapaxes(\n",
        "    1,2).reshape(-1, D, 2) # [T*M, D, 2]\n",
        "\n",
        "# Plot\n",
        "def spec(N):                                             \n",
        "    t = np.linspace(-510, 510, N)                                              \n",
        "    return np.clip(np.stack([-t, 510-np.abs(t), t], axis=1), 0, 255)/255.\n",
        "colours = spec(D) # [D, 3]\n",
        "colours = np.stack([colours for i in range(env_vars.shape[0])]) # [T*M, D, 3]\n",
        "colours = colours.reshape(-1, 3) # [T*M*D, 3]\n",
        "env_vars = env_vars.reshape(-1, 2) # [T*M*D, 2]\n",
        "plt.scatter(x=env_vars[:, 0], y=env_vars[:, 1], c=colours)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}