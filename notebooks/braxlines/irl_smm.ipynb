{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssCOanHc8JH_"
      },
      "source": [
        "# Training Adversarial Inverse RL and State Marginal Matching Agents in Brax\n",
        "\n",
        "In [Brax Training](https://colab.research.google.com/github/google/brax/blob/main/notebooks/training.ipynb) we tried out [gym](https://gym.openai.com/)-like environments and PPO, SAC, evolutionary search, and trajectory optimization algorithms. We can build various RL algorithms on top of these ultra-fast implementations. This colab runs a family of [adversarial inverse RL](https://arxiv.org/abs/1911.02256) algorithms, which includes [GAIL](https://papers.nips.cc/paper/2016/hash/cc7e2b878868cbae992d1fb743995d8f-Abstract.html) and [AIRL](https://arxiv.org/abs/1710.11248) as special cases. These algorithms minimize D(p(s,a), p\\*(s,a)) or D(p(s), p\\*(s)), the divergence D between the policy's state(-action) marginal distribution p(s,a) or p(s), and a given target distribution p\\*(s,a) or p\\*(s). As discussed in [f-MAX](https://arxiv.org/abs/1911.02256), these algorithms could also be used for [state-marginal matching](https://arxiv.org/abs/1906.05274) RL besides imitation learning. Let's try them out!\n",
        "\n",
        "This provides a bare bone implementation based on minimal modifications to the\n",
        "baseline [PPO](https://github.com/google/brax/blob/main/brax/training/ppo.py),\n",
        "enabling training in a few minutes. More features, tunings, and benchmarked results will be added soon, including:  \n",
        "* Support for training a mixture of policies \n",
        "* Examples for imitation learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYe1kc3a4Oxc"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/brax/blob/main/experimental/braxlines/notebooks/irl_smm.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sOmCoOrF0F8"
      },
      "outputs": [],
      "source": [
        "#@title Install Brax and some helper modules\n",
        "#@markdown ## ⚠️ PLEASE NOTE:\n",
        "#@markdown This colab runs best using a TPU runtime.  From the Colab menu, choose Runtime \u003e Change runtime type, then select 'TPU' in the dropdown.\n",
        "\n",
        "from IPython.display import HTML, IFrame, display, clear_output \n",
        "\n",
        "brax_url = \"https://github.com/google/brax.git@main\"\n",
        "!pip install git+$brax_url\n",
        "clear_output()\n",
        "\n",
        "from datetime import datetime\n",
        "import functools\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.tools import colab_tpu\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from typing import Any, Callable\n",
        "\n",
        "import tensorflow_probability as tfp\n",
        "import brax\n",
        "from brax import envs\n",
        "from brax.io import html\n",
        "from brax.experimental.braxlines.training import ppo\n",
        "from brax.experimental.braxlines.irl_smm import utils as irl_utils\n",
        "\n",
        "tfp = tfp.substrates.jax\n",
        "tfd = tfp.distributions\n",
        "\n",
        "# configure jax to run on tpu:\n",
        "colab_tpu.setup_tpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaJDZqhCLovU"
      },
      "outputs": [],
      "source": [
        "#@title Visualizing pre-included Brax environments\n",
        "env_name = \"ant\"  # @param ['ant', 'halfcheetah']\n",
        "exp_name = \"smm_multimode\"  # @param ['irl_indexing', 'irl', 'smm', 'smm_multimode', 'smm_multimode3', 'smm_maxent']\n",
        "algo_name = \"gail\"  # @param ['gail', 'airl']\n",
        "disc_arch = \"mlp\"  # @param ['linear', 'mlp']\n",
        "logits_clip_range = 10.0# @param {'type': 'number'}\n",
        "\n",
        "env_indices = {\n",
        "    'ant': (13, 14),  # x-y velocities\n",
        "    'humanoid': (22, 23),  # x-y velocities\n",
        "    'halfcheetah': (11,),  # x velocity\n",
        "}[env_name]\n",
        "base_env_fn = envs.create_fn(env_name=env_name)\n",
        "base_env = base_env_fn()\n",
        "env_obs_size = base_env.observation_size\n",
        "\n",
        "disc_arch = {\n",
        "    'linear': (),\n",
        "    'mlp': (32, 32),\n",
        "}[disc_arch]\n",
        "if exp_name in ['smm', 'smm_multimode', 'smm_multimode3', 'smm_maxent']:\n",
        "  disc_fn = functools.partial(\n",
        "        irl_utils.IRLDiscriminator,\n",
        "        input_size=len(env_indices),\n",
        "        obs_indices=env_indices,\n",
        "        include_action=False,\n",
        "        arch=disc_arch,\n",
        "        logits_clip_range=logits_clip_range,\n",
        "        )\n",
        "else:\n",
        "  raise NotImplementedError(exp_name)\n",
        "disc = disc_fn(reward_type=algo_name)\n",
        "extra_params = disc.init_model(rng=jax.random.PRNGKey(seed=0)) \n",
        "env_fn = irl_utils.create_fn(env_name=env_name, disc=disc)\n",
        "env = env_fn()\n",
        "\n",
        "def visualize(sys, qps):\n",
        "  \"\"\"Renders a 3D visualization of the environment.\"\"\"\n",
        "  return HTML(html.render(sys, qps))\n",
        "\n",
        "# Print out some samples from env.reset()\n",
        "for i in range(3):\n",
        "  state = env.reset(rng=jax.random.PRNGKey(seed=i))\n",
        "  print(f'Sample={i}, obs_shape={state.obs.shape}, act_size={env.action_size}')\n",
        "\n",
        "# Visualize in 3D\n",
        "visualize(env.sys, [state.qp])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rM7nNiXJU-4s"
      },
      "outputs": [],
      "source": [
        "#@title Generate and visualize target data distribution p\\*(s, a) or p\\*(s)\n",
        "N =  250# @param{type: 'integer'}\n",
        "\n",
        "def draw_2d_uniform(rng, N, low, high):\n",
        "  rng, key = jax.random.split(rng)\n",
        "  dist = tfd.Uniform(low=jnp.array(low), high=jnp.array(high))\n",
        "  data_2d = dist.sample(sample_shape=N, seed=key)\n",
        "  return rng, data_2d\n",
        "\n",
        "rng = jax.random.PRNGKey(seed=0)\n",
        "if exp_name == 'smm':\n",
        "  rng, target_data_2d = draw_2d_uniform(rng, N=N, low=[-6.,-0.5], high=[-4.,0.5])\n",
        "  target_data = target_data_2d[..., :len(env_indices)]\n",
        "elif exp_name == 'smm_multimode':\n",
        "  rng, sample1 = draw_2d_uniform(rng, N=N, low=[-6.,-0.5], high=[-4.,0.5])\n",
        "  rng, sample2 = draw_2d_uniform(rng, N=N, low=[4.,-0.5], high=[6.,0.5])\n",
        "  target_data_2d = jnp.concatenate([sample1, sample2], axis=0) \n",
        "  target_data = target_data_2d[..., :len(env_indices)]\n",
        "elif exp_name == 'smm_multimode3':\n",
        "  rng, sample1 = draw_2d_uniform(rng, N=N, low=[-2.,-2.], high=[-1.,-1.])\n",
        "  rng, sample2 = draw_2d_uniform(rng, N=N, low=[1.,-2.], high=[2.,-1.])\n",
        "  rng, sample3 = draw_2d_uniform(rng, N=N, low=[-0.5,1.], high=[0.5,2.])\n",
        "  target_data_2d = jnp.concatenate([sample1, sample2, sample3], axis=0) \n",
        "  target_data = target_data_2d[..., :len(env_indices)]\n",
        "elif exp_name == 'smm_maxent':\n",
        "  rng, target_data_2d = draw_2d_uniform(rng, N=N, low=[-6.,-6.], high=[6.,6.])\n",
        "  target_data = target_data_2d[..., :len(env_indices)]\n",
        "else:\n",
        "  raise NotImplementedError(exp_name)\n",
        "\n",
        "print(f'target_data={target_data.shape}')\n",
        "lim = jnp.max(jnp.abs(target_data_2d)) + 0.5\n",
        "plt.scatter(x=target_data_2d[:, 0],\n",
        "            y=target_data_2d[:, 1],\n",
        "            c=jnp.array([0,0,1]))\n",
        "plt.xlim((-lim, lim))\n",
        "plt.ylim((-lim, lim))\n",
        "plt.title('target (e.g. x-y velocities)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vgMSWODfyMC"
      },
      "outputs": [],
      "source": [
        "#@title Training some pre-included Brax environments\n",
        "num_timesteps_multiplier =  3# @param {type: 'integer'}\n",
        "\n",
        "# We determined some reasonable hyperparameters offline and share them here.\n",
        "n = num_timesteps_multiplier\n",
        "train_fn = {\n",
        "  'ant': functools.partial(\n",
        "      ppo.train, num_timesteps = 50_000_000*n, log_frequency = 20,\n",
        "      reward_scaling = 10, episode_length = 1000, normalize_observations = True,\n",
        "      action_repeat = 1, unroll_length = 5, num_minibatches = 32,\n",
        "      num_update_epochs = 4, discounting = 0.95, learning_rate = 3e-4,\n",
        "      entropy_cost = 1e-2, num_envs = 2048, batch_size = 1024\n",
        "  ),\n",
        "  'halfcheetah': functools.partial(\n",
        "      ppo.train, num_timesteps = 50_000_000*n, log_frequency = 10,\n",
        "      reward_scaling = 1, episode_length = 1000, normalize_observations = True,\n",
        "      action_repeat = 1, unroll_length = 20, num_minibatches = 32,\n",
        "      num_update_epochs = 8, discounting = 0.95, learning_rate = 3e-4,\n",
        "      entropy_cost = 0.001, num_envs = 2048, batch_size = 512\n",
        "  ),\n",
        "}[env_name]\n",
        "\n",
        "times = [datetime.now()]\n",
        "plotdata = {}\n",
        "plotkeys = ['eval/episode_reward', 'losses/disc_loss', 'losses/total_loss',\n",
        "            'losses/policy_loss', 'losses/value_loss', 'losses/entropy_loss']\n",
        "grid = jnp.linspace(-6.5, 6.5, 25)\n",
        "xgrid, ygrid = jnp.meshgrid(grid, grid)\n",
        "datagrid = jnp.concatenate([xgrid.reshape(-1, 1), ygrid.reshape(-1, 1)], axis=-1)\n",
        "lim = jnp.max(jnp.abs(datagrid)) + 0.5\n",
        "\n",
        "def progress(num_steps, metrics, optimizer_params):\n",
        "  times.append(datetime.now())\n",
        "  for key, v in metrics.items():\n",
        "    plotdata[key] = plotdata.get(key, dict(x=[], y=[]))\n",
        "    plotdata[key]['x'] += [num_steps]\n",
        "    plotdata[key]['y'] += [v]\n",
        "  clear_output(wait=True)\n",
        "  num_figs = len(plotkeys) + 1\n",
        "  fig, axs = plt.subplots(ncols=num_figs, figsize=(3.5*num_figs, 3))\n",
        "  # plot learning curves\n",
        "  for i, key in enumerate(plotkeys):\n",
        "    if key in plotdata:\n",
        "      axs[i].plot(plotdata[key]['x'], plotdata[key]['y'])\n",
        "    axs[i].set(xlabel='# environment steps', ylabel=key)\n",
        "    axs[i].set_xlim([0, train_fn.keywords['num_timesteps']])\n",
        "  # plot discriminator visualization\n",
        "  distgrid = disc.dist(datagrid, params=optimizer_params['extra'])\n",
        "  probsgrid = jax.nn.sigmoid(distgrid.logits)\n",
        "  print(f'disc probs: max={probsgrid.max()}, min={probsgrid.min()}')\n",
        "  colors = jnp.clip(jnp.array([[-2, 0, 2]]) * (probsgrid-0.5), a_min=0)\n",
        "  axs[-1].scatter(x=datagrid[:, 0],\n",
        "                  y=datagrid[:, 1],\n",
        "                  c=colors)\n",
        "  axs[-1].set_xlim((-lim, lim))\n",
        "  axs[-1].set_ylim((-lim, lim))\n",
        "  axs[-1].set(title='discriminator output (red=0, black=0.5, blue=1)')\n",
        "  fig.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "extra_loss_fns = dict(disc_loss=functools.partial(\n",
        "    irl_utils.disc_loss_fn, disc=disc, target_data=target_data,\n",
        "    balance_data=True))\n",
        "inference_fn, params, _ = train_fn(environment_fn=env_fn, \n",
        "    progress_fn=progress, extra_params=extra_params,\n",
        "    extra_loss_fns=extra_loss_fns)\n",
        "\n",
        "print(f'time to jit: {times[1] - times[0]}')\n",
        "print(f'time to train: {times[-1] - times[1]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNMLEyaTspEM"
      },
      "outputs": [],
      "source": [
        "#@title Visualizing a trajectory of the learned inference function\n",
        "seed = 0  # @param {'type': 'integer'}\n",
        "save_path = '/tmp/{env_name}_{exp_name}_{algo_name}.html' # @param {'type': 'raw'}\n",
        "save_path = save_path.format(env_name=env_name, exp_name=exp_name, algo_name=algo_name)\n",
        "\n",
        "jit_env_step = jax.jit(env.step)\n",
        "jit_inference_fn = jax.jit(inference_fn)\n",
        "qps, states = [], []\n",
        "state = env.reset(rng=jax.random.PRNGKey(seed=seed))\n",
        "while not state.done:\n",
        "  qps.append(state.qp)\n",
        "  states.append(state)\n",
        "  act = jit_inference_fn(params, state.obs, state.rng)\n",
        "  state = jit_env_step(state, act, params[-1])\n",
        "if save_path:\n",
        "  html.save_html(save_path, env.sys, qps)\n",
        "visualize(env.sys, qps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5eWOxg7RmQQ"
      },
      "outputs": [],
      "source": [
        "#@title Visualizing skills of the learned inference function in 2D plot\n",
        "import numpy as np\n",
        "from itertools import product\n",
        "num_samples = 5  # @param {type: 'integer'}\n",
        "time_subsampling = 10  # @param {type: 'integer'}\n",
        "time_last_n = 500 # @param {type: 'integer'}\n",
        "seed = 0  # @param {type: 'integer'}\n",
        "\n",
        "# Reset and run environment\n",
        "batch_env = env_fn(batch_size=num_samples)\n",
        "state = batch_env.reset(\n",
        "    jnp.array([jax.random.PRNGKey(seed+i) for i in range(num_samples)]))\n",
        "states = [state]\n",
        "jit_step = jax.jit(batch_env.step)\n",
        "jit_inference_fn = jax.jit(inference_fn)\n",
        "while not state.done.all():\n",
        "  act = jit_inference_fn(params, state.obs, state.rng[0])\n",
        "  state = jit_step(state, act, params[-1])\n",
        "  states.append(state)\n",
        "\n",
        "# Get indices of interest\n",
        "obses = jnp.stack([state.obs for state in states],\n",
        "                  axis=0)[-time_last_n:][::time_subsampling]\n",
        "env_vars = obses[..., env_indices] # [T, num_samples, #env_indices]\n",
        "target_vars = target_data\n",
        "if env_vars.shape[-1] == 1:\n",
        "  env_vars = jnp.concatenate([env_vars, jnp.zeros(env_vars.shape)], axis=-1)\n",
        "  target_vars = jnp.concatenate([target_vars, jnp.zeros(target_vars.shape)], axis=-1)\n",
        "env_vars = env_vars.reshape(-1, 2)\n",
        "target_vars = target_vars.reshape(-1, 2)\n",
        "print(f'env_vars.shape={env_vars.shape}')\n",
        "print(f'target_vars.shape={target_vars.shape}')\n",
        "\n",
        "# Plot\n",
        "lim = jnp.max(jnp.abs(\n",
        "    jnp.concatenate([env_vars, target_vars], axis=0))) + 0.5\n",
        "fig, axs = plt.subplots(ncols=2, figsize=(3.5*2, 3))\n",
        "fig.tight_layout()\n",
        "axs[0].set(title='agent policy (e.g. x-y velocities)')\n",
        "axs[0].set_xlim([-lim, lim])\n",
        "axs[0].set_ylim([-lim, lim])\n",
        "axs[0].scatter(x=env_vars[:, 0], y=env_vars[:, 1], c=[1,0,0])\n",
        "axs[1].set(title='target (e.g. x-y velocities)')\n",
        "axs[1].set_xlim([-lim, lim])\n",
        "axs[1].set_ylim([-lim, lim])\n",
        "axs[1].scatter(x=target_vars[:, 0], y=target_vars[:, 1], c=[0,0,1])\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "name": "irl_smm.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1VwIb25nv6nJT52pSuZn4ldtAWKGvENwl",
          "timestamp": 1628880272209
        },
        {
          "file_id": "1Gu8SgV7reDUv8weq2P6PRq_YQ2f88Ahv",
          "timestamp": 1628752357019
        },
        {
          "file_id": "1ZaAO4BS2tJ_03CIXdBCFibZR2yLl6dtv",
          "timestamp": 1628294539853
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
