{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "import functools\n",
    "import time # Added for PRNGKey seeding in rollout\n",
    "\n",
    "import os\n",
    "\n",
    "import jax\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import mujoco\n",
    "from mujoco import mjx\n",
    "\n",
    "from brax import envs\n",
    "from brax.base import State as PipelineState\n",
    "from brax.envs.base import Env, PipelineEnv, State as BraxState # Alias Brax's State\n",
    "from brax.mjx.base import State as MjxState\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "from brax.training.agents.ppo_lagrange import train as ppo_lagrange\n",
    "from brax.training.agents.ppo_lagrange_v2 import train as ppo_lagrange_v2\n",
    "from brax.io import html, mjcf, model as brax_model # Alias brax model\n",
    "from brax.io import json as brax_json # Added for saving trajectories if needed\n",
    "import wandb\n",
    "from ml_collections import config_dict\n",
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if subprocess.run('nvidia-smi').returncode:\n",
    "    raise RuntimeError('Cannot communicate with GPU. Make sure you have NVIDIA drivers installed.')\n",
    "\n",
    "# Configure MuJoCo to use the EGL rendering backend (requires GPU)\n",
    "os.environ['MUJOCO_GL'] = 'egl'\n",
    "\n",
    "try:\n",
    "    print('Checking that the installation succeeded:')\n",
    "    mujoco.MjModel.from_xml_string('<mujoco/>')\n",
    "except Exception as e:\n",
    "    raise e from RuntimeError(\n",
    "        'Something went wrong during installation. Check the error message above '\n",
    "        'for more information.')\n",
    "\n",
    "print('Installation successful.')\n",
    "\n",
    "# Tell XLA to use Triton GEMM, this improves steps/sec by ~30% on some GPUs\n",
    "xla_flags = os.environ.get('XLA_FLAGS', '')\n",
    "xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
    "os.environ['XLA_FLAGS'] = xla_flags\n",
    "\n",
    "# Check for GPU availability\n",
    "if subprocess.run('nvidia-smi').returncode:\n",
    "    raise RuntimeError('Cannot communicate with GPU. Make sure you have NVIDIA drivers installed.')\n",
    "\n",
    "# Configure MuJoCo to use the EGL rendering backend (requires GPU)\n",
    "os.environ['MUJOCO_GL'] = 'egl'\n",
    "\n",
    "try:\n",
    "    print('Checking that the installation succeeded:')\n",
    "    mujoco.MjModel.from_xml_string('<mujoco/>')\n",
    "except Exception as e:\n",
    "    raise e from RuntimeError(\n",
    "        'Something went wrong during installation. Check the error message above '\n",
    "        'for more information.')\n",
    "\n",
    "print('Installation successful.')\n",
    "\n",
    "# Tell XLA to use Triton GEMM, this improves steps/sec by ~30% on some GPUs\n",
    "xla_flags = os.environ.get('XLA_FLAGS', '')\n",
    "xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
    "os.environ['XLA_FLAGS'] = xla_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_config() -> config_dict.ConfigDict:\n",
    "  \"\"\"Returns the default config for PointHazardGoal environment.\"\"\"\n",
    "  config = config_dict.create(\n",
    "      # New safety-gymnasium reward parameters\n",
    "      reward_distance=3,  # Match successful PPO config\n",
    "      reward_goal=10.0,      # Match successful PPO config\n",
    "      goal_size=0.7,        # Distance threshold for achieving the goal\n",
    "      reward_orientation=False, # Optional: Reward for maintaining upright orientation\n",
    "      reward_orientation_scale=0.002, # Scale for orientation reward\n",
    "      reward_orientation_body='agent', # Body to check orientation (unused if reward_orientation=False)\n",
    "      ctrl_cost_weight=0.001, # Match successful PPO config\n",
    "      hazard_size=0.7,       # Distance threshold for hazard cost\n",
    "      # Other parameters (kept or adjusted)\n",
    "      terminate_when_unhealthy=True, # Keep termination based on health\n",
    "      healthy_z_range=(0.05, 0.3),    # Keep health definition\n",
    "      reset_noise_scale=0.005,\n",
    "      exclude_current_positions_from_observation=True,\n",
    "      max_velocity=5.0,  # Keep velocity limit for calculation stability\n",
    "      debug=False,\n",
    "  )\n",
    "  return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'point_resetting_goal_random_hazard_lidar_sensor_obs'\n",
    "# metrics_list = [] # This can be removed if wandb is the primary logger for training progress\n",
    "# Instantiate the training environment\n",
    "train_environment = envs.get_environment(env_name)\n",
    "# train_environment = PPOLagrangeCostWrapper(train_environment)\n",
    "# Instantiate a separate environment for evaluation/rollout\n",
    "eval_env = envs.get_environment(env_name) # Using the same type for now\n",
    "# eval_env = PPOLagrangeCostWrapper(eval_env)\n",
    "\n",
    "print(f\"Training environment '{env_name}' instantiated.\")\n",
    "print(f\"Evaluation environment '{env_name}' instantiated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules for the wrapper\n",
    "from brax.envs.base import Wrapper\n",
    "\n",
    "# Define a custom wrapper to handle cost field properly\n",
    "class CostExtraWrapper(Wrapper):\n",
    "    \"\"\"Wrapper that moves cost from info to extras for PPO Lagrange.\"\"\"\n",
    "    \n",
    "    def step(self, state: BraxState, action: jax.Array) -> BraxState:\n",
    "        next_state = self.env.step(state, action)\n",
    "        \n",
    "        # PPO Lagrange expects cost in state.info during collection,\n",
    "        # and the training wrapper will move it to extras automatically\n",
    "        # Just ensure cost is in info\n",
    "        if 'cost' not in next_state.info:\n",
    "            # If cost is in metrics, copy it to info\n",
    "            if 'cost' in next_state.metrics:\n",
    "                next_state.info['cost'] = next_state.metrics['cost']\n",
    "            else:\n",
    "                # Default to 0 if no cost found\n",
    "                next_state.info['cost'] = jnp.zeros_like(next_state.reward)\n",
    "        \n",
    "        return next_state\n",
    "    \n",
    "    def reset(self, rng: jax.Array) -> BraxState:\n",
    "        state = self.env.reset(rng)\n",
    "        # Ensure cost is initialized in info\n",
    "        if 'cost' not in state.info:\n",
    "            state.info['cost'] = jnp.zeros_like(state.reward)\n",
    "        return state\n",
    "\n",
    "# Custom wrap function that includes the cost wrapper\n",
    "def wrap_env_with_cost(env: envs.Env) -> envs.Env:\n",
    "    \"\"\"Wrap environment with cost handling for PPO Lagrange.\"\"\"\n",
    "    return CostExtraWrapper(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_progress_fn(num_steps, metrics, metrics_list=None, use_wandb=False):\n",
    "    \"\"\"\n",
    "    Progress function to print metrics and log to Weights & Biases.\n",
    "    `metrics` dict can come from Brax's EpisodeMetricsLogger (keys like 'episode/reward')\n",
    "    or from Evaluator (keys like 'eval/episode_reward').\n",
    "    \"\"\"\n",
    "    print(f\"Step {num_steps}:\")\n",
    "    wandb_log_data = {}\n",
    "    for key, value in metrics.items():\n",
    "        log_value = value.item() if hasattr(value, 'item') else value \n",
    "        # Print lambda and cost-related metrics for debugging\n",
    "        if \"lambda\" in key or \"cost\" in key or \"constraint\" in key:\n",
    "            print(f\"  {key}: {log_value}\")\n",
    "        \n",
    "        if not (key.startswith(\"episode/\") or key.startswith(\"eval/\") or key.startswith(\"training/\")):\n",
    "             wandb_log_data[f\"training_batch/{key}\"] = log_value \n",
    "        else:\n",
    "             wandb_log_data[key] = log_value\n",
    "\n",
    "    if use_wandb and wandb.run is not None and wandb_log_data:\n",
    "        wandb.log(wandb_log_data, step=int(num_steps))\n",
    "\n",
    "    if metrics_list is not None: \n",
    "        metrics_data_local = {'step': num_steps}\n",
    "        metrics_data_local.update(metrics) \n",
    "        metrics_list.append(metrics_data_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items(): \n",
    "            setattr(self, key, value)\n",
    "\n",
    "# Training Arguments (Args class definition as before)\n",
    "args = Args(\n",
    "    num_timesteps=30_000_000,\n",
    "    num_evals=5, # This controls how many times the separate eval_env is run\n",
    "    reward_scaling=0.1,\n",
    "    episode_length=2000,\n",
    "    normalize_observations=True,\n",
    "    action_repeat=1, \n",
    "    unroll_length=8,\n",
    "    num_minibatches=32,\n",
    "    num_updates_per_batch=6,\n",
    "    discounting=0.99,\n",
    "    learning_rate=5e-4,\n",
    "    entropy_cost=5e-3,\n",
    "    num_envs=2048,\n",
    "    batch_size=1024,\n",
    "    max_devices_per_host=None,\n",
    "    seed=5,  # Match successful PPO config\n",
    "    safety_bound=0.2,\n",
    "    lagrangian_coef_rate=0.001,\n",
    "    initial_lambda_lagr=0.0,\n",
    ")\n",
    "\n",
    "# +++ CRITICAL FIX: Apply cost wrapper to BOTH training and evaluation environments +++\n",
    "# This ensures both training and evaluation use the same cost computation\n",
    "# train_environment = PPOLagrangeCostWrapper(envs.get_environment(env_name))\n",
    "# eval_env = PPOLagrangeCostWrapper(envs.get_environment(env_name))\n",
    "# print(f\"Applied PPOLagrangeCostWrapper to both training and evaluation environments\")\n",
    "# # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "# +++ Initialize Weights & Biases +++ (Same as before)\n",
    "config_dict_for_wandb = vars(args)\n",
    "env = envs.get_environment(env_name)\n",
    "current_env_config_for_wandb = default_config().to_dict()\n",
    "config_dict_for_wandb.update({\n",
    "    \"environment_name\": env_name,\n",
    "    \"reward_distance\": current_env_config_for_wandb.get(\"reward_distance\"),\n",
    "    \"reward_goal\": current_env_config_for_wandb.get(\"reward_goal\"),\n",
    "    \"goal_size\": current_env_config_for_wandb.get(\"goal_size\"),\n",
    "    \"ctrl_cost_weight\": current_env_config_for_wandb.get(\"ctrl_cost_weight\"),\n",
    "    \"reward_orientation\": current_env_config_for_wandb.get(\"reward_orientation\"),\n",
    "    \"reward_orientation_scale\": current_env_config_for_wandb.get(\"reward_orientation_scale\"),\n",
    "})\n",
    "run_name = f\"{env_name}_ppo_lag_v2_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "run = wandb.init(\n",
    "    project=\"safe_brax\",\n",
    "    name=run_name,\n",
    "    config=config_dict_for_wandb,\n",
    ")\n",
    "# ++++++++++++++++++++++++++++++++\n",
    "\n",
    "# Bound progress function (This now becomes more important for training metrics)\n",
    "metrics_list = []\n",
    "# Ensure custom_progress_fn is ready to handle metrics dictionary and log to wandb\n",
    "# (Definition of custom_progress_fn shown after this cell block)\n",
    "bound_progress_fn = functools.partial(custom_progress_fn, metrics_list=metrics_list, use_wandb=True)\n",
    "\n",
    "# Setup the PPO training function\n",
    "train_fn = functools.partial(\n",
    "    ppo_lagrange_v2,\n",
    "    num_timesteps=args.num_timesteps,\n",
    "    num_evals=args.num_evals, # For separate evaluations using eval_env\n",
    "    reward_scaling=args.reward_scaling,\n",
    "    episode_length=args.episode_length,\n",
    "    normalize_observations=args.normalize_observations,\n",
    "    action_repeat=args.action_repeat,\n",
    "    unroll_length=args.unroll_length,\n",
    "    num_minibatches=args.num_minibatches,\n",
    "    num_updates_per_batch=args.num_updates_per_batch,\n",
    "    learning_rate=args.learning_rate,\n",
    "    entropy_cost=args.entropy_cost,\n",
    "    discounting=args.discounting,\n",
    "    num_envs=args.num_envs,\n",
    "    batch_size=args.batch_size,\n",
    "    max_devices_per_host=args.max_devices_per_host,\n",
    "    seed=args.seed,\n",
    "    # +++ Brax's new training metrics logging +++\n",
    "    log_training_metrics=True, # Enable Brax's internal episode metrics logging\n",
    "    training_metrics_steps=args.episode_length * args.num_envs / 5, # Example: log ~5 times per \"epoch\" of data collection\n",
    "    # Or set to a fixed number like 100000, or None to use default\n",
    "    # +++++++++++++++++++++++++++++++++++++++++++++\n",
    "    safety_bound=args.safety_bound,\n",
    "    lagrangian_coef_rate=args.lagrangian_coef_rate,\n",
    "    initial_lambda_lagr=args.initial_lambda_lagr,\n",
    ")\n",
    "print(\"Training arguments and PPO train_fn configured. Weights & Biases run initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Starting PPO-Lagrange training for {env_name}...\")\n",
    "make_inference_fn, params, final_eval_metrics = train_fn( # Renamed for clarity\n",
    "    environment=train_environment,\n",
    "    eval_env=eval_env, # Pass the separate eval_environment\n",
    "    progress_fn=bound_progress_fn\n",
    ")\n",
    "print(\"Training finished.\")\n",
    "print(f\"Final evaluation metrics: {final_eval_metrics}\")\n",
    "\n",
    "# +++ Log final evaluation metrics to W&B +++\n",
    "if wandb.run is not None and final_eval_metrics:\n",
    "    # These metrics should already be prefixed with 'eval/' by evaluator.run_evaluation\n",
    "    # if not, you might need to add it.\n",
    "    # Let's assume they are correctly prefixed or are self-descriptive.\n",
    "    final_log_data = {}\n",
    "    for key, value in final_eval_metrics.items():\n",
    "        log_value = value.item() if hasattr(value, 'item') else value\n",
    "        # If keys from final_eval_metrics are not prefixed with 'eval/', add it\n",
    "        # Example: if key is 'episode_reward', log as 'final_eval/episode_reward'\n",
    "        # if not key.startswith(\"eval/\"):\n",
    "        #     final_log_data[f\"final_evaluation/{key}\"] = log_value\n",
    "        # else:\n",
    "        final_log_data[key] = log_value # Assuming keys are like 'eval/episode_reward'\n",
    "\n",
    "    wandb.log(final_log_data, step=int(args.num_timesteps)) # Log at the final step\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# metrics_filename = f\"metrics/training_metrics_notebook_{env_name}_{timestamp}.csv\"\n",
    "# os.makedirs('metrics', exist_ok=True)\n",
    "# with open(metrics_filename, 'w', newline='') as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     # ... (write headers and data from metrics_list as in your save_brax_metrics) ...\n",
    "# print(f\"Metrics saved to {metrics_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model (params)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_path = f'models/{env_name.lower()}_lag_notebook_{timestamp}'\n",
    "os.makedirs('models', exist_ok=True)\n",
    "brax_model.save_params(model_path, params)\n",
    "print(f\"Trained model parameters saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a separate environment for evaluation/rollout\n",
    "eval_env_name = env_name # Or the same env_name\n",
    "eval_environment = envs.get_environment(eval_env_name)\n",
    "\n",
    "# jit reset and step for the eval environment\n",
    "jit_eval_reset = jax.jit(eval_environment.reset)\n",
    "jit_eval_step = jax.jit(eval_environment.step)\n",
    "\n",
    "# Create the actual inference function using the factory and loaded params\n",
    "# `params` here is the 3-tuple: (normalizer_params, policy_network_weights, value_network_weights)\n",
    "# `make_inference_fn` knows how to use these.\n",
    "inference_fn = make_inference_fn(params)\n",
    "jit_inference_fn = jax.jit(inference_fn)\n",
    "\n",
    "print(f\"Inference function for rollout created for {eval_env_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rollout_steps = 5000\n",
    "rollout_frames = []\n",
    "\n",
    "# Data collection for plotting (expanded)\n",
    "rollout_metrics_data = {\n",
    "    'distance_to_goal': [],\n",
    "    'last_dist_goal': [], # New\n",
    "    'reward': [],\n",
    "    'dist_reward': [],       # New\n",
    "    'goal_reward': [],       # New\n",
    "    'orientation_reward': [],# New\n",
    "    'ctrl_cost': [],         # New\n",
    "    'x_position': [],\n",
    "    'y_position': [],\n",
    "    'agent_pos_x': [],       # New\n",
    "    'agent_pos_y': [],       # New\n",
    "    'goal_pos_x': [],        # New\n",
    "    'goal_pos_y': [],        # New\n",
    "    'x_velocity': [],\n",
    "    'y_velocity': [],\n",
    "    'goals_reached_count': [],\n",
    "    'cost': []\n",
    "}\n",
    "\n",
    "actions = []\n",
    "\n",
    "rng_rollout = jax.random.PRNGKey(int(time.time()))\n",
    "eval_state = jit_eval_reset(rng_rollout)\n",
    "\n",
    "print(f\"Starting rollout for {num_rollout_steps} steps...\")\n",
    "for i in range(num_rollout_steps):\n",
    "    act_rng, rng_rollout = jax.random.split(rng_rollout)\n",
    "    action, _ = jit_inference_fn(eval_state.obs, act_rng)\n",
    "    actions.append(action)\n",
    "    \n",
    "    # Store agent_pos and goal_pos *before* the step, if they are needed for \"last_goal_pos\" type logic\n",
    "    # For current step's agent_pos and goal_pos, they are in eval_state.info *after* the step.\n",
    "\n",
    "    eval_state = jit_eval_step(eval_state, action)\n",
    "    rollout_frames.append(eval_state.pipeline_state)\n",
    "\n",
    "    # Collect metrics from eval_state.metrics\n",
    "    rollout_metrics_data['distance_to_goal'].append(eval_state.metrics.get('distance_to_goal', np.nan))\n",
    "    rollout_metrics_data['reward'].append(eval_state.metrics.get('reward', np.nan))\n",
    "    rollout_metrics_data['cost'].append(eval_state.metrics.get('cost', np.nan))\n",
    "    rollout_metrics_data['dist_reward'].append(eval_state.metrics.get('dist_reward', np.nan))\n",
    "    rollout_metrics_data['goal_reward'].append(eval_state.metrics.get('goal_reward', np.nan))\n",
    "    rollout_metrics_data['orientation_reward'].append(eval_state.metrics.get('orientation_reward', np.nan))\n",
    "    rollout_metrics_data['ctrl_cost'].append(eval_state.metrics.get('ctrl_cost'))\n",
    "    rollout_metrics_data['x_position'].append(eval_state.metrics.get('x_position', np.nan))\n",
    "    rollout_metrics_data['y_position'].append(eval_state.metrics.get('y_position', np.nan))\n",
    "    rollout_metrics_data['x_velocity'].append(eval_state.metrics.get('x_velocity', np.nan))\n",
    "    rollout_metrics_data['y_velocity'].append(eval_state.metrics.get('y_velocity', np.nan))\n",
    "    rollout_metrics_data['goals_reached_count'].append(eval_state.metrics.get('goals_reached_count', np.nan))\n",
    "\n",
    "    # Collect metrics from eval_state.info (these are from the *current* state after the step)\n",
    "    rollout_metrics_data['last_dist_goal'].append(eval_state.info.get('last_dist_goal', np.nan))\n",
    "    current_agent_pos = eval_state.info.get('agent_pos', np.array([np.nan, np.nan, np.nan]))\n",
    "    current_goal_pos = eval_state.info.get('goal_pos', np.array([np.nan, np.nan, np.nan]))\n",
    "    rollout_metrics_data['agent_pos_x'].append(current_agent_pos[0])\n",
    "    rollout_metrics_data['agent_pos_y'].append(current_agent_pos[1])\n",
    "    rollout_metrics_data['goal_pos_x'].append(current_goal_pos[0])\n",
    "    rollout_metrics_data['goal_pos_y'].append(current_goal_pos[1])\n",
    "    \n",
    "    if i % 100 == 0 or i == num_rollout_steps - 1:\n",
    "        print(f\"Rollout step {i+1}/{num_rollout_steps} completed. Goals reached: {eval_state.metrics.get('goals_reached_count', 0)}\")\n",
    "\n",
    "    if eval_state.done:\n",
    "        print(f\"Rollout terminated early at step {i+1} due to done signal.\")\n",
    "        remaining_steps = num_rollout_steps - (i + 1)\n",
    "        for key_metric in rollout_metrics_data.keys():\n",
    "            rollout_metrics_data[key_metric].extend([np.nan] * remaining_steps)\n",
    "        break\n",
    "print(\"Rollout finished.\")\n",
    "\n",
    "# Save trajectory as JSON (optional)\n",
    "trajectory_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "os.makedirs('trajectories', exist_ok=True)\n",
    "rollout_trajectory_path = f'trajectories/{eval_env_name}_lag_rollout_notebook.json'\n",
    "brax_json.save(rollout_trajectory_path, eval_environment.sys, rollout_frames)\n",
    "print(f\"Rollout trajectory saved to {rollout_trajectory_path}\")\n",
    "\n",
    "# Render video (optional)\n",
    "# video_html = HTML(html.render(eval_environment.sys, rollout_frames))\n",
    "# video_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# os.makedirs('videos', exist_ok=True)\n",
    "# media.write_video(f'videos/{eval_env_name}_rollout_notebook_{video_timestamp}.mp4', \n",
    "#                   eval_environment.render(rollout_frames, camera='side'),\n",
    "#                   fps=1.0 / eval_environment.dt)\n",
    "# print(f\"Rollout video saved.\")\n",
    "# video_html\n",
    "\n",
    "# save metrics to wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_rollout_bug(rollout_metrics_data):\n",
    "    \"\"\"Analyze the rollout data to find the distance reward bug.\"\"\"\n",
    "    print(\"=== Analyzing Rollout Distance Reward Bug ===\")\n",
    "    \n",
    "    # Extract data\n",
    "    distance_to_goal = np.array(rollout_metrics_data['distance_to_goal'])\n",
    "    last_dist_goal = np.array(rollout_metrics_data['last_dist_goal'])\n",
    "    dist_reward = np.array(rollout_metrics_data['dist_reward'])\n",
    "    goal_reward = np.array(rollout_metrics_data['goal_reward'])\n",
    "    \n",
    "    # Find goal achievement steps\n",
    "    goal_steps = np.where(goal_reward > 0)[0]\n",
    "    print(f\"Goal achieved at steps: {goal_steps}\")\n",
    "    \n",
    "    # Focus on the problem area - after first goal achievement\n",
    "    if len(goal_steps) > 0:\n",
    "        problem_start = goal_steps[0]\n",
    "        \n",
    "        print(f\"\\nAnalyzing steps around goal reset (step {problem_start}):\")\n",
    "        \n",
    "        for i in range(max(0, problem_start-2), min(problem_start + 10, len(distance_to_goal))):\n",
    "            # Manual calculation of expected distance reward\n",
    "            if i > 0:\n",
    "                expected_dist_reward = last_dist_goal[i] - distance_to_goal[i]\n",
    "            else:\n",
    "                expected_dist_reward = 0.0\n",
    "                \n",
    "            actual_dist_reward = dist_reward[i]\n",
    "            \n",
    "            marker = \" <-- GOAL!\" if i == problem_start else \"\"\n",
    "            print(f\"  Step {i}: dist_to_goal={distance_to_goal[i]:.3f}, \"\n",
    "                  f\"last_dist_goal={last_dist_goal[i]:.3f}, \"\n",
    "                  f\"expected_reward={expected_dist_reward:.4f}, \"\n",
    "                  f\"actual_reward={actual_dist_reward:.4f}{marker}\")\n",
    "    \n",
    "    # Check if last_dist_goal equals distance_to_goal\n",
    "    diff = np.abs(last_dist_goal - distance_to_goal)\n",
    "    max_diff = np.max(diff)\n",
    "    steps_with_diff = np.sum(diff > 1e-4)\n",
    "    \n",
    "    print(f\"\\nComparison of last_dist_goal vs distance_to_goal:\")\n",
    "    print(f\"  Max difference: {max_diff:.6f}\")\n",
    "    print(f\"  Steps with significant difference (>1e-4): {steps_with_diff}\")\n",
    "    print(f\"  Are they essentially identical? {max_diff < 1e-3}\")\n",
    "    \n",
    "    # If they're identical, that's the bug!\n",
    "    if max_diff < 1e-3:\n",
    "        print(f\"\\n🚨 BUG FOUND: last_dist_goal ≈ distance_to_goal\")\n",
    "        print(f\"   This makes dist_reward = (last_dist_goal - distance_to_goal) ≈ 0\")\n",
    "        print(f\"   The agent can move freely without distance penalties!\")\n",
    "    \n",
    "    # Check the actual values during the major movement\n",
    "    if len(goal_steps) > 0:\n",
    "        start_step = goal_steps[0] + 1\n",
    "        end_step = min(start_step + 50, len(distance_to_goal))\n",
    "        \n",
    "        print(f\"\\nDuring major movement (steps {start_step}-{end_step}):\")\n",
    "        movement_distances = distance_to_goal[end_step-1] - distance_to_goal[start_step]\n",
    "        total_dist_rewards = np.sum(dist_reward[start_step:end_step])\n",
    "        \n",
    "        print(f\"  Distance change: {distance_to_goal[start_step]:.2f} → {distance_to_goal[end_step-1]:.2f} \"\n",
    "              f\"(Δ = {movement_distances:.2f})\")\n",
    "        print(f\"  Total distance rewards: {total_dist_rewards:.4f}\")\n",
    "        print(f\"  Expected total if working: ≈ {-movement_distances:.2f}\")\n",
    "        \n",
    "        if abs(total_dist_rewards) < 0.1 and abs(movement_distances) > 10:\n",
    "            print(f\"  🚨 CONFIRMED: Large movement ({movement_distances:.1f}) with tiny rewards ({total_dist_rewards:.4f})\")\n",
    "\n",
    "# Run the analysis\n",
    "analyze_rollout_bug(rollout_metrics_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "plot_dir = 'plots'\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "plot_path_base = f'{plot_dir}/{eval_env_name}_lag_rollout_notebook_{plot_timestamp}'\n",
    "num_actual_rollout_steps = len(rollout_metrics_data['distance_to_goal']) # Use a consistent metric for length\n",
    "time_steps_rollout = np.arange(num_actual_rollout_steps)\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# Plot 1: Distance and Last Distance to Goal\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(time_steps_rollout, rollout_metrics_data['distance_to_goal'], label='Current Distance to Goal (metrics)', linestyle='-')\n",
    "plt.plot(time_steps_rollout, rollout_metrics_data['last_dist_goal'], label='Last Distance to Goal (info)', linestyle='--')\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.title(f\"{eval_env_name} - Rollout: Goal Tracking\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "goal_tracking_plot_path = f'{plot_path_base}_goal_distances.png'\n",
    "plt.savefig(goal_tracking_plot_path)\n",
    "plt.show()\n",
    "plt.close()\n",
    "print(f\"Goal tracking plot saved to: {goal_tracking_plot_path}\")\n",
    "\n",
    "# Plot: Cost Plot \n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(time_steps_rollout, rollout_metrics_data['cost'], label='Cost', linestyle='-')\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(f\"{eval_env_name} - Rollout: Cost\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "cost_plot_path = f'{plot_path_base}_cost.png'\n",
    "plt.savefig(cost_plot_path)\n",
    "plt.show()\n",
    "plt.close()\n",
    "print(f\"Cost plot saved to: {cost_plot_path}\")\n",
    "\n",
    "# Cumulative Cost Plot\n",
    "cumulative_cost = np.cumsum(rollout_metrics_data['cost'])\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(time_steps_rollout, cumulative_cost, label='Cumulative Cost', color='red')\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Cumulative Cost\")\n",
    "plt.title(f\"{eval_env_name} - Rollout: Cumulative Cost Over Time\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "cumulative_cost_plot_path = f'{plot_path_base}_cumulative_cost.png'\n",
    "plt.savefig(cumulative_cost_plot_path)\n",
    "plt.show()\n",
    "plt.close()\n",
    "print(f\"Cumulative cost plot saved to: {cumulative_cost_plot_path}\")\n",
    "\n",
    "\n",
    "# Plot 2: Reward Component Breakdown\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(time_steps_rollout, rollout_metrics_data['dist_reward'], label='Distance Reward', alpha=0.7)\n",
    "plt.plot(time_steps_rollout, rollout_metrics_data['goal_reward'], label='Goal Reward', alpha=0.7)\n",
    "plt.plot(time_steps_rollout, rollout_metrics_data['orientation_reward'], label='Orientation Reward', alpha=0.7)\n",
    "plt.plot(time_steps_rollout, -np.array(rollout_metrics_data['ctrl_cost']), label='Negative Control Cost', alpha=0.7) # Plotting as negative\n",
    "plt.plot(time_steps_rollout, rollout_metrics_data['reward'], label='Total Reward', linestyle='--', color='black', linewidth=2)\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Reward Value\")\n",
    "plt.title(f\"{eval_env_name} - Rollout: Reward Component Breakdown\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "reward_breakdown_plot_path = f'{plot_path_base}_reward_breakdown.png'\n",
    "plt.savefig(reward_breakdown_plot_path)\n",
    "plt.show()\n",
    "plt.close()\n",
    "print(f\"Reward breakdown plot saved to: {reward_breakdown_plot_path}\")\n",
    "\n",
    "# Plot 3: Agent and Goal Positions (X and Y over time)\n",
    "fig, axs = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "# X Positions\n",
    "axs[0].plot(time_steps_rollout, rollout_metrics_data['agent_pos_x'], label='Agent X Position (info)', linestyle='-')\n",
    "axs[0].plot(time_steps_rollout, rollout_metrics_data['goal_pos_x'], label='Goal X Position (info)', linestyle='--')\n",
    "axs[0].set_ylabel(\"X Position\")\n",
    "axs[0].set_title(f\"{eval_env_name} - Agent and Goal X Positions Over Time\")\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "\n",
    "# Y Positions\n",
    "axs[1].plot(time_steps_rollout, rollout_metrics_data['agent_pos_y'], label='Agent Y Position (info)', linestyle='-')\n",
    "axs[1].plot(time_steps_rollout, rollout_metrics_data['goal_pos_y'], label='Goal Y Position (info)', linestyle='--')\n",
    "axs[1].set_xlabel(\"Time Step\")\n",
    "axs[1].set_ylabel(\"Y Position\")\n",
    "axs[1].set_title(f\"{eval_env_name} - Agent and Goal Y Positions Over Time\")\n",
    "axs[1].legend()\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "agent_goal_pos_plot_path = f'{plot_path_base}_agent_goal_positions.png'\n",
    "plt.savefig(agent_goal_pos_plot_path)\n",
    "plt.show()\n",
    "plt.close()\n",
    "print(f\"Agent and Goal positions plot saved to: {agent_goal_pos_plot_path}\")\n",
    "\n",
    "\n",
    "# Keep existing X-Y Trajectory Plot (or merge agent/goal start/end points if desired)\n",
    "plt.figure(figsize=(10, 8))\n",
    "valid_x = np.array(rollout_metrics_data['x_position']) # from metrics (agent body)\n",
    "valid_y = np.array(rollout_metrics_data['y_position']) # from metrics (agent body)\n",
    "goal_x_series = np.array(rollout_metrics_data['goal_pos_x']) # from info\n",
    "goal_y_series = np.array(rollout_metrics_data['goal_pos_y']) # from info\n",
    "\n",
    "# Filter out NaNs for agent path\n",
    "valid_indices_agent = ~(np.isnan(valid_x) | np.isnan(valid_y))\n",
    "valid_x_agent = valid_x[valid_indices_agent]\n",
    "valid_y_agent = valid_y[valid_indices_agent]\n",
    "\n",
    "# Filter out NaNs for goal path (if goal moves)\n",
    "valid_indices_goal = ~(np.isnan(goal_x_series) | np.isnan(goal_y_series))\n",
    "valid_x_goal = goal_x_series[valid_indices_goal]\n",
    "valid_y_goal = goal_y_series[valid_indices_goal]\n",
    "\n",
    "\n",
    "if len(valid_x_agent) > 0 and len(valid_y_agent) > 0:\n",
    "    plt.plot(valid_x_agent, valid_y_agent, 'k-', alpha=0.7, label='Agent Path')\n",
    "    plt.scatter(valid_x_agent[0], valid_y_agent[0], c='green', s=100, label='Agent Start', zorder=5, marker='o')\n",
    "    plt.scatter(valid_x_agent[-1], valid_y_agent[-1], c='red', s=100, label='Agent End', zorder=5, marker='x')\n",
    "    \n",
    "    if len(valid_x_goal) > 0 and len(valid_y_goal) > 0:\n",
    "        # Plot goal path if it changes, or just start/end points\n",
    "        # For a fixed goal, goal_x_series[0] and goal_y_series[0] would be the goal position\n",
    "        plt.scatter(valid_x_goal[0], valid_y_goal[0], c='blue', s=150, label='Initial Goal', zorder=4, marker='*')\n",
    "        if any(g_x != valid_x_goal[0] for g_x in valid_x_goal) or any(g_y != valid_y_goal[0] for g_y in valid_y_goal):\n",
    "             plt.plot(valid_x_goal, valid_y_goal, 'b--', alpha=0.5, label='Goal Path (if dynamic)')\n",
    "             plt.scatter(valid_x_goal[-1], valid_y_goal[-1], c='purple', s=150, label='Final Goal', zorder=4, marker='*')\n",
    "\n",
    "\n",
    "    plt.xlabel(\"X Position\")\n",
    "    plt.ylabel(\"Y Position\")\n",
    "    plt.title(f\"{eval_env_name} - Rollout: X-Y Trajectory with Goal(s)\")\n",
    "    plt.legend()\n",
    "    plt.axis('equal')\n",
    "    plt.grid(True)\n",
    "else:\n",
    "    plt.text(0.5, 0.5, \"No valid position data for trajectory plot\", ha='center', va='center')\n",
    "plt.tight_layout()\n",
    "trajectory_plot_path_updated = f'{plot_path_base}_xy_trajectory_with_goals.png'\n",
    "plt.savefig(trajectory_plot_path_updated)\n",
    "plt.show()\n",
    "plt.close()\n",
    "print(f\"X-Y trajectory plot with goals saved to: {trajectory_plot_path_updated}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Hyperparameter Tuning Diagnostic Checklist\n",
    "\n",
    "## 1. Performance Health Check\n",
    "- [ ] **Episode Reward**: Is it increasing? Target: > 0 after 5M steps\n",
    "- [ ] **Goals Reached**: Are any goals being reached? Target: > 0.5 per episode\n",
    "- [ ] **Distance Reward**: Is it positive on average? Target: > 0\n",
    "\n",
    "## 2. Safety Balance Check  \n",
    "- [ ] **Cost Return**: Is it near cost_limit? Target: within 20% of limit\n",
    "- [ ] **Lambda**: Has it stabilized? Target: 0.1 - 2.0 range\n",
    "- [ ] **Constraint Violation**: Is it converging to 0? Target: |violation| < 0.05\n",
    "\n",
    "## 3. Learning Dynamics Check\n",
    "- [ ] **Policy Loss**: Is it decreasing? Check for plateaus\n",
    "- [ ] **Value Loss**: Both reward and cost V-loss < 1.0?\n",
    "- [ ] **Entropy**: Is it > 0.001? (not collapsing)\n",
    "\n",
    "## 4. Trajectory Analysis\n",
    "- [ ] **Movement Pattern**: Circular? Straight? Avoiding hazards?\n",
    "- [ ] **Goal Approach**: Is distance to goal decreasing?\n",
    "- [ ] **Hazard Interaction**: Appropriate avoidance?\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# PPO-Lagrange Hyperparameter Quick Reference\n",
    "\n",
    "## Key Hyperparameters & Their Effects\n",
    "\n",
    "### 1. **reward_scaling** (default: 1.0, range: 0.1-5.0)\n",
    "- ↑ Higher = More aggressive goal-seeking\n",
    "- ↓ Lower = More cautious behavior\n",
    "- **Tune when**: Goals not reached or too many hazard hits\n",
    "\n",
    "### 2. **cost_limit** (default: 0.25, range: 0.05-0.5)\n",
    "- ↑ Higher = More permissive (allows more hazard contact)\n",
    "- ↓ Lower = Stricter safety requirements\n",
    "- **Tune when**: Lambda grows too high or agent is too conservative\n",
    "\n",
    "### 3. **lambda_lr** (default: 0.001, range: 0.0001-0.1)\n",
    "- ↑ Higher = Faster constraint adaptation\n",
    "- ↓ Lower = Slower, more stable constraint enforcement\n",
    "- **Tune when**: Lambda oscillates or grows too fast/slow\n",
    "\n",
    "### 4. **entropy_cost** (default: 1e-3, range: 1e-4 to 1e-1)\n",
    "- ↑ Higher = More exploration\n",
    "- ↓ Lower = More exploitation\n",
    "- **Tune when**: Agent stuck in local patterns or entropy < 0.001\n",
    "\n",
    "### 5. **lambda_init** (default: 0.0, range: 0.0-1.0)\n",
    "- Higher = Start with safety bias\n",
    "- Lower = Allow initial exploration\n",
    "- **Tune when**: Early training has too many violations\n",
    "\n",
    "## Tuning Decision Tree\n",
    "\n",
    "```\n",
    "Is agent reaching goals?\n",
    "├─ NO\n",
    "│  ├─ Is distance_reward negative?\n",
    "│  │  └─ YES → Check environment implementation\n",
    "│  │  └─ NO → Is lambda > 2?\n",
    "│  │       └─ YES → ↑cost_limit, ↓lambda_lr\n",
    "│  │       └─ NO → ↑entropy_cost, ↑reward_scaling\n",
    "│  └─ \n",
    "└─ YES\n",
    "   ├─ Are costs within limit?\n",
    "   │  └─ NO → ↑lambda_lr, ↓reward_scaling\n",
    "   │  └─ YES → Minor tuning only\n",
    "   └─ \n",
    "```\n",
    "\n",
    "## Common Patterns & Solutions\n",
    "\n",
    "| Pattern | Metrics | Solution |\n",
    "|---------|---------|----------|\n",
    "| Circular motion | goals=0, dist_reward<0 | Fix env transforms |\n",
    "| Freezing | lambda>3, low costs | ↑cost_limit, ↓lambda_lr |\n",
    "| Reckless | high costs, lambda at max | ↓reward_scaling, ↑lambda_lr |\n",
    "| No exploration | entropy<0.001 | ↑entropy_cost significantly |\n",
    "| Unstable | oscillating metrics | ↓learning_rate, ↓lambda_lr |\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Critical Fix Applied!\n",
    "\n",
    "**Bug Found**: PPO-Lagrange was applying observation normalization **twice**:\n",
    "1. First in `preprocess_observations_fn`\n",
    "2. Then again in `policy_network.apply`\n",
    "\n",
    "This caused observations to be normalized to near-zero values, making the agent output constant actions (pirouettes).\n",
    "\n",
    "**Fix**: Modified `brax/training/agents/ppo_lagrange/networks.py` to remove the double normalization.\n",
    "\n",
    "Now PPO-Lagrange should behave like regular PPO when lambda=0 or cost_limit is very high!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Automated hyperparameter grid search for systematic tuning\n",
    "\n",
    "# def create_hyperparameter_grid():\n",
    "#     \"\"\"Create a grid of hyperparameters to try based on common issues.\"\"\"\n",
    "    \n",
    "#     # Base configuration\n",
    "#     base_config = {\n",
    "#         'num_timesteps': 5_000_000,  # Shorter for quick tests\n",
    "#         'num_evals': 10,\n",
    "#         'episode_length': 2000,\n",
    "#         'normalize_observations': True,\n",
    "#         'action_repeat': 1,\n",
    "#         'unroll_length': 10,\n",
    "#         'num_minibatches': 32,\n",
    "#         'num_updates_per_batch': 8,\n",
    "#         'discounting': 0.97,\n",
    "#         'num_envs': 2048,\n",
    "#         'batch_size': 512,\n",
    "#         'clipping_epsilon': 0.2,\n",
    "#     }\n",
    "    \n",
    "#     # Hyperparameter grid for different scenarios\n",
    "#     grid = {\n",
    "#         # Conservative baseline\n",
    "#         'conservative': {\n",
    "#             **base_config,\n",
    "#             'reward_scaling': 1.0,\n",
    "#             'cost_limit': 0.3,\n",
    "#             'lambda_init': 0.1,\n",
    "#             'lambda_lr': 0.01,\n",
    "#             'lambda_max': 5.0,\n",
    "#             'entropy_cost': 5e-3,\n",
    "#             'learning_rate': 3e-4,\n",
    "#         },\n",
    "        \n",
    "#         # Aggressive exploration\n",
    "#         'exploratory': {\n",
    "#             **base_config,\n",
    "#             'reward_scaling': 3.0,\n",
    "#             'cost_limit': 0.3,\n",
    "#             'lambda_init': 0.0,\n",
    "#             'lambda_lr': 0.001,\n",
    "#             'lambda_max': 2.0,\n",
    "#             'entropy_cost': 2e-2,\n",
    "#             'learning_rate': 5e-4,\n",
    "#         },\n",
    "        \n",
    "#         # Balanced approach\n",
    "#         'balanced': {\n",
    "#             **base_config,\n",
    "#             'reward_scaling': 2.0,\n",
    "#             'cost_limit': 0.25,\n",
    "#             'lambda_init': 0.0,\n",
    "#             'lambda_lr': 0.005,\n",
    "#             'lambda_max': 3.0,\n",
    "#             'entropy_cost': 1e-2,\n",
    "#             'learning_rate': 3e-4,\n",
    "#         },\n",
    "        \n",
    "#         # High safety\n",
    "#         'safety_first': {\n",
    "#             **base_config,\n",
    "#             'reward_scaling': 1.5,\n",
    "#             'cost_limit': 0.15,\n",
    "#             'lambda_init': 0.5,\n",
    "#             'lambda_lr': 0.02,\n",
    "#             'lambda_max': 10.0,\n",
    "#             'entropy_cost': 5e-3,\n",
    "#             'learning_rate': 3e-4,\n",
    "#         },\n",
    "        \n",
    "#         # Curriculum (start easy, constraint later)\n",
    "#         'curriculum': {\n",
    "#             **base_config,\n",
    "#             'reward_scaling': 2.5,\n",
    "#             'cost_limit': 0.4,  # Very permissive initially\n",
    "#             'lambda_init': 0.0,\n",
    "#             'lambda_lr': 0.0001,  # Very slow growth\n",
    "#             'lambda_max': 2.0,\n",
    "#             'entropy_cost': 1.5e-2,\n",
    "#             'learning_rate': 4e-4,\n",
    "#         }\n",
    "#     }\n",
    "    \n",
    "#     return grid\n",
    "\n",
    "# def run_hyperparameter_experiments(grid, env_name, num_seeds=3):\n",
    "#     \"\"\"Run experiments with different hyperparameter configurations.\"\"\"\n",
    "    \n",
    "#     results = {}\n",
    "    \n",
    "#     for config_name, config in grid.items():\n",
    "#         print(f\"\\n{'='*60}\")\n",
    "#         print(f\"Running configuration: {config_name}\")\n",
    "#         print(f\"{'='*60}\")\n",
    "        \n",
    "#         config_results = []\n",
    "        \n",
    "#         for seed in range(num_seeds):\n",
    "#             print(f\"\\nSeed {seed}...\")\n",
    "            \n",
    "#             # Update seed in config\n",
    "#             config['seed'] = seed\n",
    "            \n",
    "#             # Create Args object\n",
    "#             exp_args = Args(**config)\n",
    "            \n",
    "#             # Create unique wandb run name\n",
    "#             run_name = f\"{env_name}_{config_name}_seed{seed}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "            \n",
    "#             # Initialize wandb\n",
    "#             run = wandb.init(\n",
    "#                 project=\"safe_brax_hyperparam_search\",\n",
    "#                 name=run_name,\n",
    "#                 config=config,\n",
    "#                 group=config_name,\n",
    "#                 reinit=True\n",
    "#             )\n",
    "            \n",
    "#             try:\n",
    "#                 # Run training (abbreviated for grid search)\n",
    "#                 train_env = envs.get_environment(env_name)\n",
    "#                 eval_env = envs.get_environment(env_name)\n",
    "                \n",
    "#                 metrics_list = []\n",
    "#                 progress_fn = functools.partial(\n",
    "#                     custom_progress_fn, \n",
    "#                     metrics_list=metrics_list, \n",
    "#                     use_wandb=True\n",
    "#                 )\n",
    "                \n",
    "#                 train_fn = functools.partial(\n",
    "#                     ppo_lagrange.train,\n",
    "#                     **{k: v for k, v in config.items() if k != 'seed'}\n",
    "#                 )\n",
    "                \n",
    "#                 make_inference_fn, params, final_metrics = train_fn(\n",
    "#                     environment=train_env,\n",
    "#                     eval_env=eval_env,\n",
    "#                     progress_fn=progress_fn\n",
    "#                 )\n",
    "                \n",
    "#                 # Extract key metrics\n",
    "#                 result = {\n",
    "#                     'seed': seed,\n",
    "#                     'final_goals': final_metrics.get('eval/episode_goals_reached_count', 0),\n",
    "#                     'final_reward': final_metrics.get('eval/episode_reward', 0),\n",
    "#                     'final_cost': final_metrics.get('training/cost_return', 0),\n",
    "#                     'final_lambda': final_metrics.get('training/lambda', 0),\n",
    "#                     'metrics_list': metrics_list\n",
    "#                 }\n",
    "                \n",
    "#                 config_results.append(result)\n",
    "                \n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error in {config_name} seed {seed}: {e}\")\n",
    "#                 config_results.append({'error': str(e)})\n",
    "            \n",
    "#             finally:\n",
    "#                 wandb.finish()\n",
    "        \n",
    "#         # Aggregate results\n",
    "#         results[config_name] = {\n",
    "#             'config': config,\n",
    "#             'runs': config_results,\n",
    "#             'avg_goals': np.mean([r['final_goals'] for r in config_results if 'final_goals' in r]),\n",
    "#             'avg_reward': np.mean([r['final_reward'] for r in config_results if 'final_reward' in r]),\n",
    "#             'avg_cost': np.mean([r['final_cost'] for r in config_results if 'final_cost' in r]),\n",
    "#         }\n",
    "    \n",
    "#     return results\n",
    "\n",
    "# # Usage example:\n",
    "# # grid = create_hyperparameter_grid()\n",
    "# # results = run_hyperparameter_experiments(grid, env_name, num_seeds=2)\n",
    "# # \n",
    "# # # Analyze results\n",
    "# # for config_name, data in results.items():\n",
    "# #     print(f\"\\n{config_name}:\")\n",
    "# #     print(f\"  Avg Goals: {data['avg_goals']:.2f}\")\n",
    "# #     print(f\"  Avg Reward: {data['avg_reward']:.2f}\")\n",
    "# #     print(f\"  Avg Cost: {data['avg_cost']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def save_metrics_list_to_csv(metrics_list, filename=\"metrics_list.csv\"):\n",
    "    \"\"\"\n",
    "    Save a list of metrics dictionaries to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        metrics_list (list): List of dictionaries containing metrics.\n",
    "        filename (str): Output CSV filename.\n",
    "    \"\"\"\n",
    "    if not metrics_list:\n",
    "        print(\"metrics_list is empty, nothing to save.\")\n",
    "        return\n",
    "\n",
    "    # Flatten nested numpy arrays and handle missing keys\n",
    "    def flatten_dict(d):\n",
    "        flat = {}\n",
    "        for k, v in d.items():\n",
    "            # Convert numpy arrays to scalars if needed\n",
    "            try:\n",
    "                if isinstance(v, np.ndarray):\n",
    "                    flat[k] = v.item()\n",
    "                else:\n",
    "                    flat[k] = v\n",
    "            except Exception:\n",
    "                flat[k] = v\n",
    "        return flat\n",
    "\n",
    "    # Collect all possible keys\n",
    "    all_keys = set()\n",
    "    for entry in metrics_list:\n",
    "        all_keys.update(flatten_dict(entry).keys())\n",
    "    all_keys = sorted(all_keys)\n",
    "\n",
    "    # Write to CSV\n",
    "    with open(filename, mode='w', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=all_keys)\n",
    "        writer.writeheader()\n",
    "        for entry in metrics_list:\n",
    "            flat_entry = flatten_dict(entry)\n",
    "            writer.writerow({k: flat_entry.get(k, \"\") for k in all_keys})\n",
    "\n",
    "    print(f\"Saved {len(metrics_list)} metric entries to {filename}\")\n",
    "\n",
    "filename = './metrics/' + run_name + \"_seed_\" + str(args.seed) + \"_metrics_list.csv\"\n",
    "# Example usage:\n",
    "save_metrics_list_to_csv(metrics_list, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display, Javascript\n",
    "import time\n",
    "\n",
    "def play_completion_sound():\n",
    "    \"\"\"\n",
    "    Plays a short notification sound in the notebook to notify the user that processing is complete.\n",
    "    Uses IPython.display.Audio to play a simple beep with forced autoplay.\n",
    "    \"\"\"\n",
    "    # Generate a 440 Hz sine wave beep for 0.5 seconds\n",
    "    fs = 22050  # Sampling rate\n",
    "    duration = 1  # seconds (longer for better audibility)\n",
    "    frequency = 500 # Hz\n",
    "    t = np.linspace(0, duration, int(fs * duration), False)\n",
    "    beep = 0.3 * np.sin(2 * np.pi * frequency * t)\n",
    "    \n",
    "    # Create audio with autoplay enabled\n",
    "    audio = Audio(beep, rate=fs, autoplay=True)\n",
    "    \n",
    "    # Display the audio widget with a unique ID\n",
    "    unique_id = f\"completion_sound_{int(time.time() * 1000)}\"\n",
    "    display(audio, display_id=unique_id)\n",
    "    \n",
    "    # Enhanced Javascript to force audio playback\n",
    "    display(Javascript(f\"\"\"\n",
    "    setTimeout(function() {{\n",
    "        // Try multiple selectors to find the audio element\n",
    "        var audios = document.querySelectorAll('audio');\n",
    "        var lastAudio = audios[audios.length - 1]; // Get the most recently added audio\n",
    "        \n",
    "        if (lastAudio) {{\n",
    "            lastAudio.volume = 0.2;\n",
    "            lastAudio.play().catch(function(error) {{\n",
    "                console.log('Audio autoplay failed:', error);\n",
    "                // Fallback: create a simple beep using Web Audio API\n",
    "                try {{\n",
    "                    var audioContext = new (window.AudioContext || window.webkitAudioContext)();\n",
    "                    var oscillator = audioContext.createOscillator();\n",
    "                    var gainNode = audioContext.createGain();\n",
    "                    \n",
    "                    oscillator.connect(gainNode);\n",
    "                    gainNode.connect(audioContext.destination);\n",
    "                    \n",
    "                    oscillator.frequency.value = 5;\n",
    "                    oscillator.type = 'sine';\n",
    "                    gainNode.gain.setValueAtTime(0.1, audioContext.currentTime);\n",
    "                    gainNode.gain.exponentialRampToValueAtTime(0.01, audioContext.currentTime + 0.5);\n",
    "                    \n",
    "                    oscillator.start(audioContext.currentTime);\n",
    "                    oscillator.stop(audioContext.currentTime + 0.5);\n",
    "                }} catch(e) {{\n",
    "                    console.log('Web Audio API fallback also failed:', e);\n",
    "                }}\n",
    "            }});\n",
    "        }}\n",
    "    }}, 100);\n",
    "    \"\"\"))\n",
    "\n",
    "# Call the function to play the sound\n",
    "play_completion_sound()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_completion_sound()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Bayesian sweep optimizing eval/episode_reward; change to cost or a composite if desired\n",
    "sweep_config = {\n",
    "    \"name\": \"ppol_bayes_pointgoal_nb\",\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\"name\": \"episode/goals_reached_count\", \"goal\": \"maximize\"},\n",
    "    \"early_terminate\": {\"type\": \"hyperband\", \"min_iter\": 3, \"eta\": 3},\n",
    "    \"parameters\": {\n",
    "        # Fixed context for this project\n",
    "        \"env\": {\"value\": \"point_resetting_goal_random_hazard_lidar_sensor_obs\"},\n",
    "        \"alg\": {\"value\": \"ppo_lagrange\"},\n",
    "        # PPO core\n",
    "        \"learning_rate\": {\"distribution\": \"log_uniform_values\", \"min\": 1e-5, \"max\": 1e-3},\n",
    "        \"entropy_cost\": {\"distribution\": \"log_uniform_values\", \"min\": 1e-4, \"max\": 1e-2},\n",
    "        \"batch_size\": {\"values\": [256, 512, 1024]},\n",
    "        \"num_minibatches\": {\"values\": [16, 32, 64]},\n",
    "        \"num_updates_per_batch\": {\"values\": [2, 4, 8]},\n",
    "        \"unroll_length\": {\"values\": [5, 10, 20]},\n",
    "        \"gae_lambda\": {\"min\": 0.9, \"max\": 0.98},\n",
    "        \"clipping_epsilon\": {\"min\": 0.1, \"max\": 0.3},\n",
    "        # PPO-Lagrange hparams\n",
    "        \"safety_bound\": {\"value\": 0.2},\n",
    "        \"lagrangian_coef_rate\": {\"distribution\": \"log_uniform_values\", \"min\": 1e-3, \"max\": 1e-1},\n",
    "        \"initial_lambda_lagr\": {\"values\": [0.0, 0.1, 1.0]},\n",
    "        # Env overrides (optional)\n",
    "        \"env_kwargs\": {\"value\": {\"config_overrides\": {\"hazard_size\": 0.7}}},\n",
    "        # Runtime + eval\n",
    "        \"num_timesteps\": {\"value\": 30_000_000},\n",
    "        \"episode_length\": {\"value\": 1000},\n",
    "        \"num_envs\": {\"value\": 1024},\n",
    "        \"num_evals\": {\"value\": 3},\n",
    "        \"num_eval_envs\": {\"value\": 128},\n",
    "        \"deterministic_eval\": {\"value\": False},\n",
    "        \"normalize_observations\": {\"value\": True},\n",
    "        # Seeds to compare robustness\n",
    "        \"seed\": {\"value\": 243512},\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 4gqfm0w5\n",
      "Sweep URL: https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax/sweeps/4gqfm0w5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tajsjzqt with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talg: ppo_lagrange\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclipping_epsilon: 0.21600191113706177\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdeterministic_eval: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tentropy_cost: 0.006172038247113296\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tenv: point_resetting_goal_random_hazard_lidar_sensor_obs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tenv_kwargs: {'config_overrides': {'hazard_size': 0.7}}\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepisode_length: 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.9076281169918482\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_lambda_lagr: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlagrangian_coef_rate: 0.015475408878600418\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 8.252955652905625e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnormalize_observations: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_envs: 1024\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_eval_envs: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_evals: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_minibatches: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_timesteps: 30000000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_updates_per_batch: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsafety_bound: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 243512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tunroll_length: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mm-boustani\u001b[0m (\u001b[33mm-boustani-eindhoven-university-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mrdbstn/school/safe-brax/safe-brax/notebooks/wandb/run-20250825_092217-tajsjzqt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax/runs/tajsjzqt' target=\"_blank\">earthy-sweep-1</a></strong> to <a href='https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax/sweeps/4gqfm0w5' target=\"_blank\">https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax/sweeps/4gqfm0w5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax' target=\"_blank\">https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax/sweeps/4gqfm0w5' target=\"_blank\">https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax/sweeps/4gqfm0w5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax/runs/tajsjzqt' target=\"_blank\">https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax/runs/tajsjzqt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrdbstn/school/safe-brax/safe-brax/brax/io/mjcf.py:480: UserWarning: Brax System, piplines and environments are not actively being maintained. Please see MJX for a well maintained JAX-based physics engine: https://github.com/google-deepmind/mujoco/tree/main/mjx. For a host of environments that use MJX, see: https://github.com/google-deepmind/mujoco_playground.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Body 0 name: world, mocapid: [-1]\n",
      "Body 1 name: agent, mocapid: [-1]\n",
      "Body 2 name: goal, mocapid: [0]\n",
      "Goal body found with mocapid: [0]\n",
      "Body 3 name: hazard1, mocapid: [1]\n",
      "Hazard body found with mocapid: [1]\n",
      "Body 4 name: hazard2, mocapid: [2]\n",
      "Hazard body found with mocapid: [2]\n",
      "Body 5 name: hazard3, mocapid: [3]\n",
      "Hazard body found with mocapid: [3]\n",
      "Model has 7 sensors. Searching for required sensors...\n",
      "  Found sensor: accelerometer, ID: 0, Address: 0, Dim: 3\n",
      "  Found sensor: velocimeter, ID: 1, Address: 3, Dim: 3\n",
      "  Found sensor: gyro, ID: 2, Address: 6, Dim: 3\n",
      "  Found sensor: magnetometer, ID: 3, Address: 9, Dim: 3\n",
      "Body 0 name: world, mocapid: [-1]\n",
      "Body 1 name: agent, mocapid: [-1]\n",
      "Body 2 name: goal, mocapid: [0]\n",
      "Goal body found with mocapid: [0]\n",
      "Body 3 name: hazard1, mocapid: [1]\n",
      "Hazard body found with mocapid: [1]\n",
      "Body 4 name: hazard2, mocapid: [2]\n",
      "Hazard body found with mocapid: [2]\n",
      "Body 5 name: hazard3, mocapid: [3]\n",
      "Hazard body found with mocapid: [3]\n",
      "Model has 7 sensors. Searching for required sensors...\n",
      "  Found sensor: accelerometer, ID: 0, Address: 0, Dim: 3\n",
      "  Found sensor: velocimeter, ID: 1, Address: 3, Dim: 3\n",
      "  Found sensor: gyro, ID: 2, Address: 6, Dim: 3\n",
      "  Found sensor: magnetometer, ID: 3, Address: 9, Dim: 3\n",
      "Reset method - Goal Position: Traced<ShapedArray(float32[3])>with<BatchTrace> with\n",
      "  val = Traced<ShapedArray(float32[1024,3])>with<BatchTrace> with\n",
      "    val = Traced<ShapedArray(float32[1,1024,3])>with<DynamicJaxprTrace>\n",
      "    batch_dim = 0\n",
      "  batch_dim = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrdbstn/school/safe-brax/safe-brax/env/lib/python3.10/site-packages/jax/_src/interpreters/xla.py:112: RuntimeWarning: overflow encountered in cast\n",
      "  return np.asarray(x, dtypes.canonicalize_dtype(x.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset method - Goal Position: Traced<ShapedArray(float32[3])>with<BatchTrace> with\n",
      "  val = Traced<ShapedArray(float32[128,3])>with<DynamicJaxprTrace>\n",
      "  batch_dim = 0\n",
      "Reset method - Goal Position: Traced<ShapedArray(float32[3])>with<BatchTrace> with\n",
      "  val = Traced<ShapedArray(float32[128,3])>with<DynamicJaxprTrace>\n",
      "  batch_dim = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>env_steps</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>episode/cost</td><td>▆▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▄█▅▅▃▃▃▃▅▅▇▆▆▅</td></tr><tr><td>episode/ctrl_cost</td><td>█████▇▇▆▆▅▅▅▅▅▄▄▄▄▄▄▄▄▄▄▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>episode/dist_reward</td><td>▄▁▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇█████████████████</td></tr><tr><td>episode/distance_to_goal</td><td>▃▃▃█▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>episode/goal_reward</td><td>█▇▇▇▄▄▇▇▇▇▂▂▁▁▁▁▃▃▃▃▃▃▁▁▂▁▂▂▃▃▁▁▁▁▁▁▂▃▃▂</td></tr><tr><td>episode/goals_reached_count</td><td>▁▄▄▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇█████</td></tr><tr><td>episode/last_dist_goal</td><td>▃██▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>episode/length</td><td>████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>episode/orientation_reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>episode/reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>episode/sum_reward</td><td>▄▁▅▅▅▅▆▅▅▅▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇█</td></tr><tr><td>episode/x_position</td><td>▅██▆▆▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>episode/x_velocity</td><td>▇▇█▅▅▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▁▁▂▂▁▁▁▁▁▂</td></tr><tr><td>episode/y_position</td><td>▆▁▅▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███▇▇▇▇▇█████████</td></tr><tr><td>episode/y_velocity</td><td>▄▄▁▁▅▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇▇▇███████</td></tr><tr><td>episode/z_alignment</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/avg_episode_length</td><td>▁▁▁</td></tr><tr><td>eval/episode_cost</td><td>█▁▆</td></tr><tr><td>eval/episode_cost_std</td><td>▅▁█</td></tr><tr><td>eval/episode_ctrl_cost</td><td>█▄▁</td></tr><tr><td>eval/episode_ctrl_cost_std</td><td>▁█▅</td></tr><tr><td>eval/episode_dist_reward</td><td>▁██</td></tr><tr><td>eval/episode_dist_reward_std</td><td>█▃▁</td></tr><tr><td>eval/episode_distance_to_goal</td><td>█▃▁</td></tr><tr><td>eval/episode_distance_to_goal_std</td><td>█▅▁</td></tr><tr><td>eval/episode_goal_reward</td><td>▆█▁</td></tr><tr><td>eval/episode_goal_reward_std</td><td>▄█▁</td></tr><tr><td>eval/episode_goals_reached_count</td><td>▅█▁</td></tr><tr><td>eval/episode_goals_reached_count_std</td><td>▄█▁</td></tr><tr><td>eval/episode_last_dist_goal</td><td>█▃▁</td></tr><tr><td>eval/episode_last_dist_goal_std</td><td>█▅▁</td></tr><tr><td>eval/episode_orientation_reward</td><td>▁▁▁</td></tr><tr><td>eval/episode_orientation_reward_std</td><td>▁▁▁</td></tr><tr><td>eval/episode_reward</td><td>▁█▇</td></tr><tr><td>eval/episode_reward_std</td><td>█▇▁</td></tr><tr><td>eval/episode_x_position</td><td>█▄▁</td></tr><tr><td>eval/episode_x_position_std</td><td>▁█▆</td></tr><tr><td>eval/episode_x_velocity</td><td>█▄▁</td></tr><tr><td>eval/episode_x_velocity_std</td><td>▁█▃</td></tr><tr><td>eval/episode_y_position</td><td>▁▇█</td></tr><tr><td>eval/episode_y_position_std</td><td>█▅▁</td></tr><tr><td>eval/episode_y_velocity</td><td>▁▇█</td></tr><tr><td>eval/episode_y_velocity_std</td><td>█▃▁</td></tr><tr><td>eval/episode_z_alignment</td><td>▁▁▁</td></tr><tr><td>eval/episode_z_alignment_std</td><td>▁▁▁</td></tr><tr><td>eval/epoch_eval_time</td><td>█▆▁</td></tr><tr><td>eval/sps</td><td>▁▁█</td></tr><tr><td>eval/std_episode_length</td><td>▁▁▁</td></tr><tr><td>eval/walltime</td><td>▁▇█</td></tr><tr><td>training/cost_v_loss</td><td>█▁</td></tr><tr><td>training/cost_violation</td><td>▁█</td></tr><tr><td>training/entropy_loss</td><td>▁█</td></tr><tr><td>training/lambda_lagr</td><td>█▁</td></tr><tr><td>training/mean_cost</td><td>▁█</td></tr><tr><td>training/policy_loss</td><td>█▁</td></tr><tr><td>training/sps</td><td>▁█</td></tr><tr><td>training/total_loss</td><td>█▁</td></tr><tr><td>training/v_loss</td><td>█▁</td></tr><tr><td>training/walltime</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>env_steps</td><td>30064640</td></tr><tr><td>episode/cost</td><td>148.68</td></tr><tr><td>episode/ctrl_cost</td><td>0.15959</td></tr><tr><td>episode/dist_reward</td><td>0.01625</td></tr><tr><td>episode/distance_to_goal</td><td>1695.09381</td></tr><tr><td>episode/goal_reward</td><td>0.1</td></tr><tr><td>episode/goals_reached_count</td><td>763.75</td></tr><tr><td>episode/last_dist_goal</td><td>1695.09921</td></tr><tr><td>episode/length</td><td>999</td></tr><tr><td>episode/orientation_reward</td><td>0</td></tr><tr><td>episode/reward</td><td>0</td></tr><tr><td>episode/sum_reward</td><td>-0.04334</td></tr><tr><td>episode/x_position</td><td>-146.37129</td></tr><tr><td>episode/x_velocity</td><td>-26.88666</td></tr><tr><td>episode/y_position</td><td>69.34585</td></tr><tr><td>episode/y_velocity</td><td>14.74913</td></tr><tr><td>episode/z_alignment</td><td>0</td></tr><tr><td>eval/avg_episode_length</td><td>1000</td></tr><tr><td>eval/episode_cost</td><td>164.13281</td></tr><tr><td>eval/episode_cost_std</td><td>301.02136</td></tr><tr><td>eval/episode_ctrl_cost</td><td>0.13563</td></tr><tr><td>eval/episode_ctrl_cost_std</td><td>0.06371</td></tr><tr><td>eval/episode_dist_reward</td><td>0.5751</td></tr><tr><td>eval/episode_dist_reward_std</td><td>1.81465</td></tr><tr><td>eval/episode_distance_to_goal</td><td>1559.37195</td></tr><tr><td>eval/episode_distance_to_goal_std</td><td>403.85956</td></tr><tr><td>eval/episode_goal_reward</td><td>1.25</td></tr><tr><td>eval/episode_goal_reward_std</td><td>3.95285</td></tr><tr><td>eval/episode_goals_reached_count</td><td>66.5</td></tr><tr><td>eval/episode_goals_reached_count_std</td><td>209.27512</td></tr><tr><td>eval/episode_last_dist_goal</td><td>1559.56348</td></tr><tr><td>eval/episode_last_dist_goal_std</td><td>403.63245</td></tr><tr><td>eval/episode_orientation_reward</td><td>0</td></tr><tr><td>eval/episode_orientation_reward_std</td><td>0</td></tr><tr><td>eval/episode_reward</td><td>1.68946</td></tr><tr><td>eval/episode_reward_std</td><td>5.16728</td></tr><tr><td>eval/episode_x_position</td><td>-71.2442</td></tr><tr><td>eval/episode_x_position_std</td><td>662.80511</td></tr><tr><td>eval/episode_x_velocity</td><td>-11.50861</td></tr><tr><td>eval/episode_x_velocity_std</td><td>104.80202</td></tr><tr><td>eval/episode_y_position</td><td>32.37363</td></tr><tr><td>eval/episode_y_position_std</td><td>146.03595</td></tr><tr><td>eval/episode_y_velocity</td><td>5.67871</td></tr><tr><td>eval/episode_y_velocity_std</td><td>27.99537</td></tr><tr><td>eval/episode_z_alignment</td><td>0</td></tr><tr><td>eval/episode_z_alignment_std</td><td>0</td></tr><tr><td>eval/epoch_eval_time</td><td>8.6895</td></tr><tr><td>eval/sps</td><td>14730.42202</td></tr><tr><td>eval/std_episode_length</td><td>0</td></tr><tr><td>eval/walltime</td><td>171.90866</td></tr><tr><td>training/cost_v_loss</td><td>0.27551</td></tr><tr><td>training/cost_violation</td><td>-0.04163</td></tr><tr><td>training/entropy_loss</td><td>0.00084</td></tr><tr><td>training/lambda_lagr</td><td>0.04448</td></tr><tr><td>training/mean_cost</td><td>0.15837</td></tr><tr><td>training/policy_loss</td><td>-0.00215</td></tr><tr><td>training/sps</td><td>55281.00841</td></tr><tr><td>training/total_loss</td><td>0.27595</td></tr><tr><td>training/v_loss</td><td>0.00175</td></tr><tr><td>training/walltime</td><td>607.73775</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">earthy-sweep-1</strong> at: <a href='https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax/runs/tajsjzqt' target=\"_blank\">https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax/runs/tajsjzqt</a><br> View project at: <a href='https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax' target=\"_blank\">https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250825_092217-tajsjzqt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tried to log to step 30000000 that is less than the current step 30064640. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: f4obidul with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talg: ppo_lagrange\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclipping_epsilon: 0.11739169907954404\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdeterministic_eval: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tentropy_cost: 0.0007906313819905515\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tenv: point_resetting_goal_random_hazard_lidar_sensor_obs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tenv_kwargs: {'config_overrides': {'hazard_size': 0.7}}\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepisode_length: 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.9362177424349144\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_lambda_lagr: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlagrangian_coef_rate: 0.07873242402786093\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1.267672037417473e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnormalize_observations: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_envs: 1024\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_eval_envs: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_evals: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_minibatches: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_timesteps: 30000000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_updates_per_batch: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsafety_bound: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 243512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tunroll_length: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for wandb.init()..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mrdbstn/school/safe-brax/safe-brax/notebooks/wandb/run-20250825_093625-f4obidul</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax/runs/f4obidul' target=\"_blank\">peach-sweep-2</a></strong> to <a href='https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax/sweeps/4gqfm0w5' target=\"_blank\">https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax/sweeps/4gqfm0w5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax' target=\"_blank\">https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax/sweeps/4gqfm0w5' target=\"_blank\">https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax/sweeps/4gqfm0w5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax/runs/f4obidul' target=\"_blank\">https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax/runs/f4obidul</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrdbstn/school/safe-brax/safe-brax/brax/io/mjcf.py:480: UserWarning: Brax System, piplines and environments are not actively being maintained. Please see MJX for a well maintained JAX-based physics engine: https://github.com/google-deepmind/mujoco/tree/main/mjx. For a host of environments that use MJX, see: https://github.com/google-deepmind/mujoco_playground.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Body 0 name: world, mocapid: [-1]\n",
      "Body 1 name: agent, mocapid: [-1]\n",
      "Body 2 name: goal, mocapid: [0]\n",
      "Goal body found with mocapid: [0]\n",
      "Body 3 name: hazard1, mocapid: [1]\n",
      "Hazard body found with mocapid: [1]\n",
      "Body 4 name: hazard2, mocapid: [2]\n",
      "Hazard body found with mocapid: [2]\n",
      "Body 5 name: hazard3, mocapid: [3]\n",
      "Hazard body found with mocapid: [3]\n",
      "Model has 7 sensors. Searching for required sensors...\n",
      "  Found sensor: accelerometer, ID: 0, Address: 0, Dim: 3\n",
      "  Found sensor: velocimeter, ID: 1, Address: 3, Dim: 3\n",
      "  Found sensor: gyro, ID: 2, Address: 6, Dim: 3\n",
      "  Found sensor: magnetometer, ID: 3, Address: 9, Dim: 3\n",
      "Body 0 name: world, mocapid: [-1]\n",
      "Body 1 name: agent, mocapid: [-1]\n",
      "Body 2 name: goal, mocapid: [0]\n",
      "Goal body found with mocapid: [0]\n",
      "Body 3 name: hazard1, mocapid: [1]\n",
      "Hazard body found with mocapid: [1]\n",
      "Body 4 name: hazard2, mocapid: [2]\n",
      "Hazard body found with mocapid: [2]\n",
      "Body 5 name: hazard3, mocapid: [3]\n",
      "Hazard body found with mocapid: [3]\n",
      "Model has 7 sensors. Searching for required sensors...\n",
      "  Found sensor: accelerometer, ID: 0, Address: 0, Dim: 3\n",
      "  Found sensor: velocimeter, ID: 1, Address: 3, Dim: 3\n",
      "  Found sensor: gyro, ID: 2, Address: 6, Dim: 3\n",
      "  Found sensor: magnetometer, ID: 3, Address: 9, Dim: 3\n",
      "Reset method - Goal Position: Traced<ShapedArray(float32[3])>with<BatchTrace> with\n",
      "  val = Traced<ShapedArray(float32[1024,3])>with<BatchTrace> with\n",
      "    val = Traced<ShapedArray(float32[1,1024,3])>with<DynamicJaxprTrace>\n",
      "    batch_dim = 0\n",
      "  batch_dim = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrdbstn/school/safe-brax/safe-brax/env/lib/python3.10/site-packages/jax/_src/interpreters/xla.py:112: RuntimeWarning: overflow encountered in cast\n",
      "  return np.asarray(x, dtypes.canonicalize_dtype(x.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset method - Goal Position: Traced<ShapedArray(float32[3])>with<BatchTrace> with\n",
      "  val = Traced<ShapedArray(float32[128,3])>with<DynamicJaxprTrace>\n",
      "  batch_dim = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0825 09:41:05.224910  474025 pjrt_stream_executor_client.cc:2839] Execution of replica 0 failed: INTERNAL: CpuCallback error calling callback: Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 973, in _bootstrap\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "  File \"/home/mrdbstn/school/safe-brax/safe-brax/env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
      "  File \"/home/mrdbstn/school/safe-brax/safe-brax/env/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "  File \"/tmp/ipykernel_176727/291527096.py\", line 79, in train\n",
      "  File \"/home/mrdbstn/school/safe-brax/safe-brax/brax/training/agents/ppo_lagrange_v2/train.py\", line 744, in train\n",
      "  File \"/home/mrdbstn/school/safe-brax/safe-brax/brax/training/agents/ppo_lagrange_v2/train.py\", line 611, in training_epoch_with_timing\n",
      "  File \"/home/mrdbstn/school/safe-brax/safe-brax/env/lib/python3.10/site-packages/jax/_src/traceback_util.py\", line 182, in reraise_with_filtered_traceback\n",
      "  File \"/home/mrdbstn/school/safe-brax/safe-brax/env/lib/python3.10/site-packages/jax/_src/api.py\", line 1635, in cache_miss\n",
      "  File \"/home/mrdbstn/school/safe-brax/safe-brax/env/lib/python3.10/site-packages/jax/_src/profiler.py\", line 334, in wrapper\n",
      "  File \"/home/mrdbstn/school/safe-brax/safe-brax/env/lib/python3.10/site-packages/jax/_src/interpreters/pxla.py\", line 1289, in __call__\n",
      "  File \"/home/mrdbstn/school/safe-brax/safe-brax/env/lib/python3.10/site-packages/jax/_src/callback.py\", line 810, in _wrapped_callback\n",
      "Exception: \n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_176727/291527096.py\", line 79, in train\n",
      "    make_inference_fn, params, final_eval_metrics = train_fn(\n",
      "  File \"/home/mrdbstn/school/safe-brax/safe-brax/brax/training/agents/ppo_lagrange_v2/train.py\", line 744, in train\n",
      "    training_epoch_with_timing(training_state, env_state, epoch_keys)\n",
      "  File \"/home/mrdbstn/school/safe-brax/safe-brax/brax/training/agents/ppo_lagrange_v2/train.py\", line 611, in training_epoch_with_timing\n",
      "    result = training_epoch(training_state, env_state, key)\n",
      "  File \"/home/mrdbstn/school/safe-brax/safe-brax/env/lib/python3.10/site-packages/jax/_src/traceback_util.py\", line 182, in reraise_with_filtered_traceback\n",
      "    return fun(*args, **kwargs)\n",
      "  File \"/home/mrdbstn/school/safe-brax/safe-brax/env/lib/python3.10/site-packages/jax/_src/api.py\", line 1635, in cache_miss\n",
      "    out = execute(*p.flat_args)\n",
      "  File \"/home/mrdbstn/school/safe-brax/safe-brax/env/lib/python3.10/site-packages/jax/_src/profiler.py\", line 334, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/mrdbstn/school/safe-brax/safe-brax/env/lib/python3.10/site-packages/jax/_src/interpreters/pxla.py\", line 1289, in __call__\n",
      "    results = self.xla_executable.execute_sharded(\n",
      "jaxlib.xla_extension.XlaRuntimeError: INTERNAL: CpuCallback error calling callback: Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 973, in _bootstrap\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "  File \"/home/mrdbstn/school/safe-brax/safe-brax/env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
      "  File \"/home/mrdbstn/school/safe-brax/safe-brax/env/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "  File \"/tmp/ipykernel_176727/291527096.py\", line 79, in train\n",
      "  File \"/home/mrdbstn/school/safe-brax/safe-brax/brax/training/agents/ppo_lagrange_v2/train.py\", line 744, in train\n",
      "  File \"/home/mrdbstn/school/safe-brax/safe-brax/brax/training/agents/ppo_lagrange_v2/train.py\", line 611, in training_epoch_with_timing\n",
      "  File \"/home/mrdbstn/school/safe-brax/safe-brax/env/lib/python3.10/site-packages/jax/_src/traceback_util.py\", line 182, in reraise_with_filtered_traceback\n",
      "  File \"/home/mrdbstn/school/safe-brax/safe-brax/env/lib/python3.10/site-packages/jax/_src/api.py\", line 1635, in cache_miss\n",
      "  File \"/home/mrdbstn/school/safe-brax/safe-brax/env/lib/python3.10/site-packages/jax/_src/profiler.py\", line 334, in wrapper\n",
      "  File \"/home/mrdbstn/school/safe-brax/safe-brax/env/lib/python3.10/site-packages/jax/_src/interpreters/pxla.py\", line 1289, in __call__\n",
      "  File \"/home/mrdbstn/school/safe-brax/safe-brax/env/lib/python3.10/site-packages/jax/_src/callback.py\", line 810, in _wrapped_callback\n",
      "Exception: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>env_steps</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>episode/cost</td><td>█████████████████████████▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>episode/ctrl_cost</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████████</td></tr><tr><td>episode/dist_reward</td><td>█████████████████████████▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>episode/distance_to_goal</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████████</td></tr><tr><td>episode/goal_reward</td><td>█████████████████████████▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>episode/goals_reached_count</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████████</td></tr><tr><td>episode/last_dist_goal</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████████</td></tr><tr><td>episode/length</td><td>█████████████████████████▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>episode/orientation_reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>episode/reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>episode/sum_reward</td><td>█████████████████████████▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>episode/x_position</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████████</td></tr><tr><td>episode/x_velocity</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████████</td></tr><tr><td>episode/y_position</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████████</td></tr><tr><td>episode/y_velocity</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████████</td></tr><tr><td>episode/z_alignment</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/avg_episode_length</td><td>▁</td></tr><tr><td>eval/episode_cost</td><td>▁</td></tr><tr><td>eval/episode_cost_std</td><td>▁</td></tr><tr><td>eval/episode_ctrl_cost</td><td>▁</td></tr><tr><td>eval/episode_ctrl_cost_std</td><td>▁</td></tr><tr><td>eval/episode_dist_reward</td><td>▁</td></tr><tr><td>eval/episode_dist_reward_std</td><td>▁</td></tr><tr><td>eval/episode_distance_to_goal</td><td>▁</td></tr><tr><td>eval/episode_distance_to_goal_std</td><td>▁</td></tr><tr><td>eval/episode_goal_reward</td><td>▁</td></tr><tr><td>eval/episode_goal_reward_std</td><td>▁</td></tr><tr><td>eval/episode_goals_reached_count</td><td>▁</td></tr><tr><td>eval/episode_goals_reached_count_std</td><td>▁</td></tr><tr><td>eval/episode_last_dist_goal</td><td>▁</td></tr><tr><td>eval/episode_last_dist_goal_std</td><td>▁</td></tr><tr><td>eval/episode_orientation_reward</td><td>▁</td></tr><tr><td>eval/episode_orientation_reward_std</td><td>▁</td></tr><tr><td>eval/episode_reward</td><td>▁</td></tr><tr><td>eval/episode_reward_std</td><td>▁</td></tr><tr><td>eval/episode_x_position</td><td>▁</td></tr><tr><td>eval/episode_x_position_std</td><td>▁</td></tr><tr><td>eval/episode_x_velocity</td><td>▁</td></tr><tr><td>eval/episode_x_velocity_std</td><td>▁</td></tr><tr><td>eval/episode_y_position</td><td>▁</td></tr><tr><td>eval/episode_y_position_std</td><td>▁</td></tr><tr><td>eval/episode_y_velocity</td><td>▁</td></tr><tr><td>eval/episode_y_velocity_std</td><td>▁</td></tr><tr><td>eval/episode_z_alignment</td><td>▁</td></tr><tr><td>eval/episode_z_alignment_std</td><td>▁</td></tr><tr><td>eval/epoch_eval_time</td><td>▁</td></tr><tr><td>eval/sps</td><td>▁</td></tr><tr><td>eval/std_episode_length</td><td>▁</td></tr><tr><td>eval/walltime</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>env_steps</td><td>2498560</td></tr><tr><td>episode/cost</td><td>207.26</td></tr><tr><td>episode/ctrl_cost</td><td>0.56136</td></tr><tr><td>episode/dist_reward</td><td>-1.60944</td></tr><tr><td>episode/distance_to_goal</td><td>1843.4392</td></tr><tr><td>episode/goal_reward</td><td>0</td></tr><tr><td>episode/goals_reached_count</td><td>29.97</td></tr><tr><td>episode/last_dist_goal</td><td>1842.90271</td></tr><tr><td>episode/length</td><td>999</td></tr><tr><td>episode/orientation_reward</td><td>0</td></tr><tr><td>episode/reward</td><td>0</td></tr><tr><td>episode/sum_reward</td><td>-2.1708</td></tr><tr><td>episode/x_position</td><td>495.64058</td></tr><tr><td>episode/x_velocity</td><td>137.24166</td></tr><tr><td>episode/y_position</td><td>49.60855</td></tr><tr><td>episode/y_velocity</td><td>34.98212</td></tr><tr><td>episode/z_alignment</td><td>0</td></tr><tr><td>eval/avg_episode_length</td><td>1000</td></tr><tr><td>eval/episode_cost</td><td>195.04688</td></tr><tr><td>eval/episode_cost_std</td><td>218.05847</td></tr><tr><td>eval/episode_ctrl_cost</td><td>0.64391</td></tr><tr><td>eval/episode_ctrl_cost_std</td><td>0.01406</td></tr><tr><td>eval/episode_dist_reward</td><td>-3.96169</td></tr><tr><td>eval/episode_dist_reward_std</td><td>3.47734</td></tr><tr><td>eval/episode_distance_to_goal</td><td>2350.75</td></tr><tr><td>eval/episode_distance_to_goal_std</td><td>641.65686</td></tr><tr><td>eval/episode_goal_reward</td><td>2.03125</td></tr><tr><td>eval/episode_goal_reward_std</td><td>4.39449</td></tr><tr><td>eval/episode_goals_reached_count</td><td>117.89062</td></tr><tr><td>eval/episode_goals_reached_count_std</td><td>266.9198</td></tr><tr><td>eval/episode_last_dist_goal</td><td>2349.4292</td></tr><tr><td>eval/episode_last_dist_goal_std</td><td>640.90009</td></tr><tr><td>eval/episode_orientation_reward</td><td>0</td></tr><tr><td>eval/episode_orientation_reward_std</td><td>0</td></tr><tr><td>eval/episode_reward</td><td>-2.57435</td></tr><tr><td>eval/episode_reward_std</td><td>6.73163</td></tr><tr><td>eval/episode_x_position</td><td>1083.97021</td></tr><tr><td>eval/episode_x_position_std</td><td>306.3754</td></tr><tr><td>eval/episode_x_velocity</td><td>186.81935</td></tr><tr><td>eval/episode_x_velocity_std</td><td>97.63066</td></tr><tr><td>eval/episode_y_position</td><td>-793.86926</td></tr><tr><td>eval/episode_y_position_std</td><td>238.53026</td></tr><tr><td>eval/episode_y_velocity</td><td>-267.44348</td></tr><tr><td>eval/episode_y_velocity_std</td><td>59.82761</td></tr><tr><td>eval/episode_z_alignment</td><td>0</td></tr><tr><td>eval/episode_z_alignment_std</td><td>0</td></tr><tr><td>eval/epoch_eval_time</td><td>90.80746</td></tr><tr><td>eval/sps</td><td>1409.57585</td></tr><tr><td>eval/std_episode_length</td><td>0</td></tr><tr><td>eval/walltime</td><td>90.80746</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">peach-sweep-2</strong> at: <a href='https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax/runs/f4obidul' target=\"_blank\">https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax/runs/f4obidul</a><br> View project at: <a href='https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax' target=\"_blank\">https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250825_093625-f4obidul/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: j3i5vnif with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talg: ppo_lagrange\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclipping_epsilon: 0.27612835142968584\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdeterministic_eval: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tentropy_cost: 0.00019619198590600128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tenv: point_resetting_goal_random_hazard_lidar_sensor_obs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tenv_kwargs: {'config_overrides': {'hazard_size': 0.7}}\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepisode_length: 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.9371334851850563\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_lambda_lagr: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlagrangian_coef_rate: 0.010989660630117222\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 6.650888753766045e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnormalize_observations: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_envs: 1024\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_eval_envs: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_evals: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_minibatches: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_timesteps: 30000000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_updates_per_batch: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsafety_bound: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 243512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tunroll_length: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for wandb.init()..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mrdbstn/school/safe-brax/safe-brax/notebooks/wandb/run-20250825_094113-j3i5vnif</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax/runs/j3i5vnif' target=\"_blank\">wobbly-sweep-3</a></strong> to <a href='https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax/sweeps/4gqfm0w5' target=\"_blank\">https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax/sweeps/4gqfm0w5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax' target=\"_blank\">https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax/sweeps/4gqfm0w5' target=\"_blank\">https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax/sweeps/4gqfm0w5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax/runs/j3i5vnif' target=\"_blank\">https://wandb.ai/m-boustani-eindhoven-university-of-technology/safe_brax/runs/j3i5vnif</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrdbstn/school/safe-brax/safe-brax/brax/io/mjcf.py:480: UserWarning: Brax System, piplines and environments are not actively being maintained. Please see MJX for a well maintained JAX-based physics engine: https://github.com/google-deepmind/mujoco/tree/main/mjx. For a host of environments that use MJX, see: https://github.com/google-deepmind/mujoco_playground.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Body 0 name: world, mocapid: [-1]\n",
      "Body 1 name: agent, mocapid: [-1]\n",
      "Body 2 name: goal, mocapid: [0]\n",
      "Goal body found with mocapid: [0]\n",
      "Body 3 name: hazard1, mocapid: [1]\n",
      "Hazard body found with mocapid: [1]\n",
      "Body 4 name: hazard2, mocapid: [2]\n",
      "Hazard body found with mocapid: [2]\n",
      "Body 5 name: hazard3, mocapid: [3]\n",
      "Hazard body found with mocapid: [3]\n",
      "Model has 7 sensors. Searching for required sensors...\n",
      "  Found sensor: accelerometer, ID: 0, Address: 0, Dim: 3\n",
      "  Found sensor: velocimeter, ID: 1, Address: 3, Dim: 3\n",
      "  Found sensor: gyro, ID: 2, Address: 6, Dim: 3\n",
      "  Found sensor: magnetometer, ID: 3, Address: 9, Dim: 3\n",
      "Body 0 name: world, mocapid: [-1]\n",
      "Body 1 name: agent, mocapid: [-1]\n",
      "Body 2 name: goal, mocapid: [0]\n",
      "Goal body found with mocapid: [0]\n",
      "Body 3 name: hazard1, mocapid: [1]\n",
      "Hazard body found with mocapid: [1]\n",
      "Body 4 name: hazard2, mocapid: [2]\n",
      "Hazard body found with mocapid: [2]\n",
      "Body 5 name: hazard3, mocapid: [3]\n",
      "Hazard body found with mocapid: [3]\n",
      "Model has 7 sensors. Searching for required sensors...\n",
      "  Found sensor: accelerometer, ID: 0, Address: 0, Dim: 3\n",
      "  Found sensor: velocimeter, ID: 1, Address: 3, Dim: 3\n",
      "  Found sensor: gyro, ID: 2, Address: 6, Dim: 3\n",
      "  Found sensor: magnetometer, ID: 3, Address: 9, Dim: 3\n",
      "Reset method - Goal Position: Traced<ShapedArray(float32[3])>with<BatchTrace> with\n",
      "  val = Traced<ShapedArray(float32[1024,3])>with<BatchTrace> with\n",
      "    val = Traced<ShapedArray(float32[1,1024,3])>with<DynamicJaxprTrace>\n",
      "    batch_dim = 0\n",
      "  batch_dim = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrdbstn/school/safe-brax/safe-brax/env/lib/python3.10/site-packages/jax/_src/interpreters/xla.py:112: RuntimeWarning: overflow encountered in cast\n",
      "  return np.asarray(x, dtypes.canonicalize_dtype(x.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset method - Goal Position: Traced<ShapedArray(float32[3])>with<BatchTrace> with\n",
      "  val = Traced<ShapedArray(float32[128,3])>with<DynamicJaxprTrace>\n",
      "  batch_dim = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import functools\n",
    "import jax\n",
    "from brax import envs\n",
    "from brax.training.agents.ppo_lagrange_v2 import train as ppo_lagrange_v2\n",
    "import wandb\n",
    "# Uses imports and definitions already present in this notebook:\n",
    "# - ppo_lagrange_v2 (imported as train)\n",
    "# - custom_progress_fn (logs to wandb and prints)\n",
    "# - env construction logic is reproduced here to be self-contained per run\n",
    "\n",
    "# Fallback logging if custom_progress_fn wasn't executed in this session\n",
    "if 'custom_progress_fn' not in globals():\n",
    "    def custom_progress_fn(num_steps, metrics, metrics_list=None, use_wandb=True):\n",
    "        wandb_log = {\"env_steps\": int(num_steps)}\n",
    "        for k, v in (metrics or {}).items():\n",
    "            try:\n",
    "                wandb_log[k] = float(v)\n",
    "            except Exception:\n",
    "                continue\n",
    "        if use_wandb:\n",
    "            wandb.log(wandb_log, step=int(num_steps))\n",
    "        if isinstance(metrics_list, list):\n",
    "            # store a shallow float-only copy\n",
    "            flat = {}\n",
    "            for k, v in (metrics or {}).items():\n",
    "                try:\n",
    "                    flat[k] = float(v)\n",
    "                except Exception:\n",
    "                    pass\n",
    "            metrics_list.append(flat)\n",
    "\n",
    "\n",
    "def train():\n",
    "    # Do not pass project explicitly; the sweep sets it and avoids the warning\n",
    "    with wandb.init() as run:\n",
    "        c = wandb.config\n",
    "\n",
    "        # Create train/eval envs with optional overrides from sweep config\n",
    "        env_kwargs = c.get(\"env_kwargs\", None) or {}\n",
    "        train_environment = envs.get_environment(c.env, **env_kwargs)\n",
    "        eval_env = envs.get_environment(c.env, **env_kwargs)\n",
    "\n",
    "        # Bind progress fn for this run (fresh list so runs don't leak state)\n",
    "        metrics_list = []\n",
    "        bound_progress_fn = functools.partial(custom_progress_fn, metrics_list=metrics_list, use_wandb=True)\n",
    "\n",
    "        # Build training callable with sweep hyperparams\n",
    "        train_fn = functools.partial(\n",
    "            ppo_lagrange_v2,  # you imported: from brax.training.agents.ppo_lagrange_v2 import train as ppo_lagrange_v2\n",
    "            num_timesteps=int(c.num_timesteps),\n",
    "            num_evals=int(c.num_evals),\n",
    "            num_eval_envs=int(c.num_eval_envs),\n",
    "            deterministic_eval=bool(c.deterministic_eval),\n",
    "            episode_length=int(c.episode_length),\n",
    "            num_envs=int(c.num_envs),\n",
    "            action_repeat=int(getattr(c, \"action_repeat\", 1)),\n",
    "            unroll_length=int(c.unroll_length),\n",
    "            batch_size=int(c.batch_size),\n",
    "            num_minibatches=int(c.num_minibatches),\n",
    "            num_updates_per_batch=int(c.num_updates_per_batch),\n",
    "            learning_rate=float(c.learning_rate),\n",
    "            entropy_cost=float(c.entropy_cost),\n",
    "            discounting=float(getattr(c, \"discounting\", 0.99)),\n",
    "            reward_scaling=float(getattr(c, \"reward_scaling\", 1.0)),\n",
    "            gae_lambda=float(c.gae_lambda),\n",
    "            clipping_epsilon=float(c.clipping_epsilon),\n",
    "            normalize_observations=bool(c.normalize_observations),\n",
    "            safety_bound=float(c.safety_bound),\n",
    "            lagrangian_coef_rate=float(c.lagrangian_coef_rate),\n",
    "            initial_lambda_lagr=float(getattr(c, \"initial_lambda_lagr\", 0.0)),\n",
    "            # Ensure we get training metrics periodically\n",
    "            log_training_metrics=True,\n",
    "            training_metrics_steps=None,\n",
    "            seed=int(c.seed),\n",
    "        )\n",
    "\n",
    "        # Train\n",
    "        make_inference_fn, params, final_eval_metrics = train_fn(\n",
    "            environment=train_environment,\n",
    "            eval_env=eval_env,\n",
    "            progress_fn=bound_progress_fn\n",
    "        )\n",
    "\n",
    "        # Log final metrics at the last step to avoid out-of-order warnings\n",
    "        if final_eval_metrics:\n",
    "            last_step = int(c.num_timesteps)\n",
    "            wandb.log({k if k.startswith(\"eval/\") else f\"eval/{k}\": float(v)\n",
    "                       for k, v in final_eval_metrics.items() if isinstance(v, (int, float))},\n",
    "                      step=last_step)\n",
    "\n",
    "# 1) Create the sweep (one-time)\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"safe_brax\")\n",
    "\n",
    "# 2) Launch agent(s). In a notebook, run count>1 to iterate configs sequentially here.\n",
    "#    For parallel workers, start multiple agents (e.g., in terminals) with the same sweep_id.\n",
    "wandb.agent(sweep_id, function=train, count=60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
