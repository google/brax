{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssCOanHc8JH_"
      },
      "source": [
        "# Training Adversarial Inverse RL and State Marginal Matching Agents in Brax\n",
        "\n",
        "In [Brax Training](https://colab.research.google.com/github/google/brax/blob/main/notebooks/training.ipynb) we tried out [gym](https://gym.openai.com/)-like environments and PPO, SAC, evolutionary search, and trajectory optimization algorithms. We can build various RL algorithms on top of these ultra-fast implementations. This colab runs a family of [adversarial inverse RL](https://arxiv.org/abs/1911.02256) algorithms, which includes [GAIL](https://papers.nips.cc/paper/2016/hash/cc7e2b878868cbae992d1fb743995d8f-Abstract.html) and [AIRL](https://arxiv.org/abs/1710.11248) as special cases. These algorithms minimize D(p(s,a), p\\*(s,a)) or D(p(s), p\\*(s)), the divergence D between the policy's state(-action) marginal distribution p(s,a) or p(s), and a given target distribution p\\*(s,a) or p\\*(s). As discussed in [f-MAX](https://arxiv.org/abs/1911.02256), these algorithms could also be used for [state-marginal matching](https://arxiv.org/abs/1906.05274) RL besides imitation learning. Let's try them out!\n",
        "\n",
        "This provides a bare bone implementation based on minimal modifications to the\n",
        "baseline [PPO](https://github.com/google/brax/blob/main/brax/training/ppo.py),\n",
        "enabling training in a few minutes. More features, tunings, and benchmarked results will be added soon, including:  \n",
        "* Support for training a mixture of policies \n",
        "* Examples for imitation learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYe1kc3a4Oxc"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/brax/blob/main/notebooks/braxlines/irl_smm.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sOmCoOrF0F8"
      },
      "outputs": [],
      "source": [
        "#@title Install Brax and some helper modules\n",
        "#@markdown ## ⚠️ PLEASE NOTE:\n",
        "#@markdown This colab runs best using a TPU runtime.  From the Colab menu, choose Runtime \u003e Change runtime type, then select 'TPU' in the dropdown.\n",
        "\n",
        "from datetime import datetime\n",
        "import functools\n",
        "import os\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from IPython.display import HTML, clear_output \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "try:\n",
        "  import brax\n",
        "except ImportError:\n",
        "  !pip install git+https://github.com/google/brax.git@main\n",
        "  clear_output()\n",
        "  import brax\n",
        "\n",
        "import tensorflow_probability as tfp\n",
        "from brax.io import html\n",
        "from brax.experimental.composer import composer\n",
        "from brax.experimental.composer import observers\n",
        "from brax.experimental.braxlines.training import ppo\n",
        "from brax.experimental.braxlines.irl_smm import utils as irl_utils\n",
        "\n",
        "tfp = tfp.substrates.jax\n",
        "tfd = tfp.distributions\n",
        "\n",
        "def visualize(sys, qps, save_path: str = None):\n",
        "  \"\"\"Renders a 3D visualization of the environment.\"\"\"\n",
        "  if save_path:\n",
        "    html.save_html(save_path, sys, qps, make_dir=True)\n",
        "  return HTML(html.render(sys, qps))\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  from jax.tools import colab_tpu\n",
        "  colab_tpu.setup_tpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaJDZqhCLovU"
      },
      "outputs": [],
      "source": [
        "#@title Visualizing pre-included Brax environments\n",
        "env_name = \"ant\"  # @param ['ant', 'halfcheetah', 'ant_cheetah', 'uni_ant', 'bi_ant']\n",
        "env_space = \"vel\" # @param ['vel', 'pos', 'ang']\n",
        "exp_name = \"smm_multimode\"  # @param ['smm', 'smm_multimode', 'smm_multimode3', 'smm_maxent']\n",
        "algo_name = \"gail2\"  # @param ['gail', 'airl', 'gail2', 'fairl']\n",
        "disc_arch = \"mlp\"  # @param ['linear', 'mlp']\n",
        "logits_clip_range = 10.0# @param {'type': 'number'}\n",
        "normalize_obs_for_disc = False # @param {'type': 'boolean'}\n",
        "balance_data_for_disc = True # @param {'type': 'boolean'}\n",
        "\n",
        "env_indices = {\n",
        "    'vel': { # x-y velocity\n",
        "      'ant': (13, 14),\n",
        "      'humanoid': (22, 23),\n",
        "      'halfcheetah': (11,),\n",
        "      'uni_ant': (('vel:torso_ant1', 0),('vel:torso_ant1', 1)),\n",
        "      'bi_ant': (('vel:torso_ant1', 0),('vel:torso_ant2', 0)),\n",
        "    },\n",
        "    'ang': { # angular velocity\n",
        "      'ant': (17,),\n",
        "      'uni_ant': (('ang:torso_ant1', 2),),\n",
        "    },\n",
        "}[env_space][env_name]\n",
        "base_env_fn = composer.create_fn(env_name=env_name)\n",
        "base_env = base_env_fn()\n",
        "\n",
        "disc_arch = {\n",
        "    'linear': (),\n",
        "    'mlp': (32, 32),\n",
        "}[disc_arch]\n",
        "if exp_name in ['smm', 'smm_multimode', 'smm_multimode3', 'smm_maxent']:\n",
        "  disc_fn = functools.partial(\n",
        "        irl_utils.IRLDiscriminator,\n",
        "        input_size=len(env_indices),\n",
        "        obs_indices=env_indices,\n",
        "        include_action=False,\n",
        "        arch=disc_arch,\n",
        "        logits_clip_range=logits_clip_range,\n",
        "        )\n",
        "else:\n",
        "  raise NotImplementedError(exp_name)\n",
        "disc = disc_fn(reward_type=algo_name,\n",
        "               normalize_obs=normalize_obs_for_disc,\n",
        "               balance_data=balance_data_for_disc,\n",
        "               env=base_env)\n",
        "extra_params = disc.init_model(rng=jax.random.PRNGKey(seed=0)) \n",
        "env_fn = irl_utils.create_fn(env_name=env_name, disc=disc)\n",
        "env = env_fn()\n",
        "\n",
        "# Visualize in 3D\n",
        "env = env_fn()\n",
        "jit_env_reset = jax.jit(env.reset)\n",
        "state = jit_env_reset(rng=jax.random.PRNGKey(seed=0))\n",
        "clear_output()  # clear out jax.lax warning before rendering\n",
        "visualize(env.sys, [state.qp])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rM7nNiXJU-4s"
      },
      "outputs": [],
      "source": [
        "#@title Generate and visualize target data distribution p\\*(s, a) or p\\*(s)\n",
        "N =  250# @param{type: 'integer'}\n",
        "\n",
        "def draw_2d_uniform(rng, N, low, high):\n",
        "  rng, key = jax.random.split(rng)\n",
        "  dist = tfd.Uniform(low=jnp.array(low), high=jnp.array(high))\n",
        "  data_2d = dist.sample(sample_shape=N, seed=key)\n",
        "  return rng, data_2d\n",
        "\n",
        "rng = jax.random.PRNGKey(seed=0)\n",
        "if exp_name == 'smm':\n",
        "  rng, target_data_2d = draw_2d_uniform(rng, N=N, low=[-6.,-0.5], high=[-4.,0.5])\n",
        "  target_data = target_data_2d[..., :len(env_indices)]\n",
        "elif exp_name == 'smm_multimode':\n",
        "  rng, sample1 = draw_2d_uniform(rng, N=N, low=[-6.,-0.5], high=[-4.,0.5])\n",
        "  rng, sample2 = draw_2d_uniform(rng, N=N, low=[4.,-0.5], high=[6.,0.5])\n",
        "  target_data_2d = jnp.concatenate([sample1, sample2], axis=0) \n",
        "  target_data = target_data_2d[..., :len(env_indices)]\n",
        "elif exp_name == 'smm_multimode3':\n",
        "  rng, sample1 = draw_2d_uniform(rng, N=N, low=[-2.,-2.], high=[-1.,-1.])\n",
        "  rng, sample2 = draw_2d_uniform(rng, N=N, low=[1.,-2.], high=[2.,-1.])\n",
        "  rng, sample3 = draw_2d_uniform(rng, N=N, low=[-0.5,1.], high=[0.5,2.])\n",
        "  target_data_2d = jnp.concatenate([sample1, sample2, sample3], axis=0) \n",
        "  target_data = target_data_2d[..., :len(env_indices)]\n",
        "elif exp_name == 'smm_maxent':\n",
        "  rng, target_data_2d = draw_2d_uniform(rng, N=N, low=[-6.,-6.], high=[6.,6.])\n",
        "  target_data = target_data_2d[..., :len(env_indices)]\n",
        "else:\n",
        "  raise NotImplementedError(exp_name)\n",
        "\n",
        "disc.set_target_data(target_data)\n",
        "\n",
        "print(f'target_data={target_data.shape}')\n",
        "lim = jnp.max(jnp.abs(target_data_2d)) + 0.5\n",
        "plt.scatter(x=target_data_2d[:, 0],\n",
        "            y=target_data_2d[:, 1],\n",
        "            c=jnp.array([0,0,1]))\n",
        "plt.xlim((-lim, lim))\n",
        "plt.ylim((-lim, lim))\n",
        "plt.title('target (e.g. x-y velocities)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vgMSWODfyMC"
      },
      "outputs": [],
      "source": [
        "#@title Training some pre-included Brax environments\n",
        "num_timesteps_multiplier =  2# @param {type: 'integer'}\n",
        "\n",
        "# We determined some reasonable hyperparameters offline and share them here.\n",
        "n = num_timesteps_multiplier\n",
        "train_fn = functools.partial(\n",
        "      ppo.train, num_timesteps = 50_000_000*n, log_frequency = 20,\n",
        "      reward_scaling = 10, episode_length = 1000, normalize_observations = True,\n",
        "      action_repeat = 1, unroll_length = 5, num_minibatches = 32,\n",
        "      num_update_epochs = 4, discounting = 0.95, learning_rate = 3e-4,\n",
        "      entropy_cost = 1e-2, num_envs = 2048, batch_size = 1024\n",
        "  )\n",
        "\n",
        "times = [datetime.now()]\n",
        "plotdata = {}\n",
        "plotkeys = ['eval/episode_reward', 'losses/disc_loss', 'losses/total_loss',\n",
        "            'losses/policy_loss', 'losses/value_loss', 'losses/entropy_loss']\n",
        "grid = jnp.linspace(-6.5, 6.5, 25)\n",
        "xgrid, ygrid = jnp.meshgrid(grid, grid)\n",
        "datagrid = jnp.concatenate([xgrid.reshape(-1, 1), ygrid.reshape(-1, 1)], axis=-1)\n",
        "lim = jnp.max(jnp.abs(datagrid)) + 0.5\n",
        "\n",
        "def progress(num_steps, metrics, optimizer_params):\n",
        "  times.append(datetime.now())\n",
        "  for key, v in metrics.items():\n",
        "    plotdata[key] = plotdata.get(key, dict(x=[], y=[]))\n",
        "    plotdata[key]['x'] += [num_steps]\n",
        "    plotdata[key]['y'] += [v]\n",
        "  clear_output(wait=True)\n",
        "  num_figs = len(plotkeys) + 1\n",
        "  fig, axs = plt.subplots(ncols=num_figs, figsize=(3.5*num_figs, 3))\n",
        "  # plot learning curves\n",
        "  for i, key in enumerate(plotkeys):\n",
        "    if key in plotdata:\n",
        "      axs[i].plot(plotdata[key]['x'], plotdata[key]['y'])\n",
        "    axs[i].set(xlabel='# environment steps', ylabel=key)\n",
        "    axs[i].set_xlim([0, train_fn.keywords['num_timesteps']])\n",
        "  # plot discriminator visualization\n",
        "  distgrid = disc.dist(datagrid[..., :len(env_indices)], params=optimizer_params['extra'])\n",
        "  probsgrid = jax.nn.sigmoid(distgrid.logits)\n",
        "  print(f'disc probs: max={probsgrid.max()}, min={probsgrid.min()}')\n",
        "  colors = jnp.clip(jnp.array([[-2, 0, 2]]) * (probsgrid-0.5), a_min=0)\n",
        "  axs[-1].scatter(x=datagrid[:, 0],\n",
        "                  y=datagrid[:, 1],\n",
        "                  c=colors)\n",
        "  axs[-1].set_xlim((-lim, lim))\n",
        "  axs[-1].set_ylim((-lim, lim))\n",
        "  axs[-1].set(title='discriminator output (red=0, black=0.5, blue=1)')\n",
        "  fig.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "extra_loss_fns = dict(disc_loss=disc.disc_loss_fn)\n",
        "inference_fn, params, _ = train_fn(environment_fn=env_fn, \n",
        "    progress_fn=progress, extra_params=extra_params,\n",
        "    extra_loss_fns=extra_loss_fns)\n",
        "\n",
        "print(f'time to jit: {times[1] - times[0]}')\n",
        "print(f'time to train: {times[-1] - times[1]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNMLEyaTspEM"
      },
      "outputs": [],
      "source": [
        "#@title Visualizing a trajectory of the learned inference function\n",
        "seed = 0  # @param {'type': 'integer'}\n",
        "save_path = None # @param {'type': 'raw'}\n",
        "save_path = save_path.format(\n",
        "    date=datetime.now().strftime('%Y%m%d'),\n",
        "    env_space=env_space,\n",
        "    env_name=env_name,\n",
        "    exp_name=exp_name,\n",
        "    algo_name=algo_name) if save_path else save_path\n",
        "\n",
        "jit_env_step = jax.jit(env.step)\n",
        "jit_inference_fn = jax.jit(inference_fn)\n",
        "qps, states = [], []\n",
        "state = env.reset(rng=jax.random.PRNGKey(seed=seed))\n",
        "while not state.done:\n",
        "  qps.append(state.qp)\n",
        "  states.append(state)\n",
        "  act = jit_inference_fn(params, state.obs, state.rng)\n",
        "  state = jit_env_step(state, act, params[0], params[-1])\n",
        "visualize(env.sys, qps, save_path=save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5eWOxg7RmQQ"
      },
      "outputs": [],
      "source": [
        "#@title Visualizing skills of the learned inference function in 2D plot\n",
        "import numpy as np\n",
        "from itertools import product\n",
        "num_samples = 5  # @param {type: 'integer'}\n",
        "time_subsampling = 10  # @param {type: 'integer'}\n",
        "time_last_n = 500 # @param {type: 'integer'}\n",
        "seed = 0  # @param {type: 'integer'}\n",
        "\n",
        "# Reset and run environment\n",
        "batch_env = env_fn(batch_size=num_samples)\n",
        "state = batch_env.reset(\n",
        "    jnp.array([jax.random.PRNGKey(seed+i) for i in range(num_samples)]))\n",
        "states = [state]\n",
        "jit_step = jax.jit(batch_env.step)\n",
        "jit_inference_fn = jax.jit(inference_fn)\n",
        "while not state.done.all():\n",
        "  act = jit_inference_fn(params, state.obs, state.rng[0])\n",
        "  state = jit_step(state, act, params[0], params[-1])\n",
        "  states.append(state)\n",
        "\n",
        "# Get indices of interest\n",
        "obses_full = jnp.stack([state.obs for state in states],\n",
        "                  axis=0)\n",
        "obses = obses_full[-time_last_n:][::time_subsampling]\n",
        "env_vars = batch_env.disc.index_obs(obses) # [T, num_samples, #env_indices]\n",
        "target_vars = target_data\n",
        "if env_vars.shape[-1] == 1:\n",
        "  env_vars = jnp.concatenate([env_vars, jnp.zeros(env_vars.shape)], axis=-1)\n",
        "  target_vars = jnp.concatenate([target_vars, jnp.zeros(target_vars.shape)], axis=-1)\n",
        "print(f'env_vars.shape={env_vars.shape}')\n",
        "print(f'target_vars.shape={target_vars.shape}')\n",
        "env_vars_flat = env_vars.reshape(-1, 2)\n",
        "target_vars_flat = target_vars.reshape(-1, 2)\n",
        "\n",
        "# Plot\n",
        "lim = jnp.max(jnp.abs(\n",
        "    jnp.concatenate([env_vars_flat, target_vars_flat], axis=0))) + 0.5\n",
        "fig, axs = plt.subplots(ncols=2, figsize=(3.5*3, 3))\n",
        "fig.tight_layout()\n",
        "axs[0].set(title='agent policy')\n",
        "axs[0].set_xlim([-lim, lim])\n",
        "axs[0].set_ylim([-lim, lim])\n",
        "axs[0].scatter(x=env_vars_flat[:, 0], y=env_vars_flat[:, 1], c=[1,0,0], alpha=0.3)\n",
        "axs[1].set(title='target')\n",
        "axs[1].set_xlim([-lim, lim])\n",
        "axs[1].set_ylim([-lim, lim])\n",
        "axs[1].scatter(x=target_vars_flat[:, 0], y=target_vars_flat[:, 1], c=[0,0,1], alpha=0.3)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "name": "irl_smm.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1VwIb25nv6nJT52pSuZn4ldtAWKGvENwl",
          "timestamp": 1629757885242
        },
        {
          "file_id": "1Gu8SgV7reDUv8weq2P6PRq_YQ2f88Ahv",
          "timestamp": 1628752357019
        },
        {
          "file_id": "1ZaAO4BS2tJ_03CIXdBCFibZR2yLl6dtv",
          "timestamp": 1628294539853
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
