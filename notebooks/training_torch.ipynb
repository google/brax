{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Brax Training with PyTorch on GPU",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trVNqxHmGISS"
      },
      "source": [
        "# Training in Brax with PyTorch on GPUs\n",
        "\n",
        "Brax is ready to integrate into other research toolkits by way of the [OpenAI Gym](https://gym.openai.com/) interface.  Brax environments convert to Gym environments using either [GymWrapper](https://github.com/google/brax/blob/main/brax/envs/wrappers.py) for single environments, or [VectorGymWrapper](https://github.com/google/brax/blob/main/brax/envs/wrappers.py) for batched (parallelized) environments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJhPpM5ZPrpq"
      },
      "source": [
        "#@title Import Brax and some helper modules\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import collections\n",
        "from datetime import datetime\n",
        "import functools\n",
        "import math\n",
        "import os\n",
        "import time\n",
        "from typing import Any, Callable, Dict, Optional, Sequence\n",
        "\n",
        "try:\n",
        "  import brax\n",
        "except ImportError:\n",
        "  !pip install git+https://github.com/google/brax.git@main\n",
        "  clear_output()\n",
        "  import brax\n",
        "\n",
        "from brax import envs\n",
        "from brax.envs import to_torch\n",
        "from brax.io import metrics\n",
        "from brax.training import ppo\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQFCkfu8Qwre"
      },
      "source": [
        "Here is a PPO Agent written in PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWJE4b5BHeH7"
      },
      "source": [
        "class Agent(nn.Module):\n",
        "  \"\"\"Standard PPO Agent with GAE and observation normalization.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               policy_layers: Sequence[int],\n",
        "               value_layers: Sequence[int],\n",
        "               entropy_cost: float,\n",
        "               discounting: float,\n",
        "               reward_scaling: float,\n",
        "               device: str):\n",
        "    super(Agent, self).__init__()\n",
        "\n",
        "    policy = []\n",
        "    for w1, w2 in zip(policy_layers, policy_layers[1:]):\n",
        "      policy.append(nn.Linear(w1, w2))\n",
        "      policy.append(nn.SiLU())\n",
        "    policy.pop()  # drop the final activation\n",
        "    self.policy = nn.Sequential(*policy)\n",
        "\n",
        "    value = []\n",
        "    for w1, w2 in zip(value_layers, value_layers[1:]):\n",
        "      value.append(nn.Linear(w1, w2))\n",
        "      value.append(nn.SiLU())\n",
        "    value.pop()  # drop the final activation\n",
        "    self.value = nn.Sequential(*value)\n",
        "\n",
        "    self.num_steps = torch.zeros((), device=device)\n",
        "    self.running_mean = torch.zeros(policy_layers[0], device=device)\n",
        "    self.running_variance = torch.zeros(policy_layers[0], device=device)\n",
        "\n",
        "    self.entropy_cost = entropy_cost\n",
        "    self.discounting = discounting\n",
        "    self.reward_scaling = reward_scaling\n",
        "    self.lambda_ = 0.95\n",
        "    self.epsilon = 0.3\n",
        "    self.device = device\n",
        "\n",
        "  @torch.jit.export\n",
        "  def dist_create(self, logits):\n",
        "    \"\"\"Normal followed by tanh.\n",
        "\n",
        "    torch.distribution doesn't work with torch.jit, so we roll our own.\"\"\"\n",
        "    loc, scale = torch.split(logits, logits.shape[-1] // 2, dim=-1)\n",
        "    scale = F.softplus(scale) + .001\n",
        "    return loc, scale\n",
        "\n",
        "  @torch.jit.export\n",
        "  def dist_sample_no_postprocess(self, loc, scale):\n",
        "    return torch.normal(loc, scale)\n",
        "\n",
        "  @classmethod\n",
        "  def dist_postprocess(cls, x):\n",
        "    return torch.tanh(x)\n",
        "\n",
        "  @torch.jit.export\n",
        "  def dist_entropy(self, loc, scale):\n",
        "    log_normalized = 0.5 * math.log(2 * math.pi) + torch.log(scale)\n",
        "    entropy = 0.5 + log_normalized\n",
        "    entropy = entropy * torch.ones_like(loc)\n",
        "    dist = torch.normal(loc, scale)\n",
        "    log_det_jacobian = 2 * (math.log(2) - dist - F.softplus(-2 * dist))\n",
        "    entropy = entropy + log_det_jacobian\n",
        "    return entropy.sum(dim=-1)\n",
        "\n",
        "  @torch.jit.export\n",
        "  def dist_log_prob(self, loc, scale, dist):\n",
        "    log_unnormalized = -0.5 * ((dist - loc) / scale).square()\n",
        "    log_normalized = 0.5 * math.log(2 * math.pi) + torch.log(scale)\n",
        "    log_det_jacobian = 2 * (math.log(2) - dist - F.softplus(-2 * dist))\n",
        "    log_prob = log_unnormalized - log_normalized - log_det_jacobian\n",
        "    return log_prob.sum(dim=-1)\n",
        "\n",
        "  @torch.jit.export\n",
        "  def update_normalization(self, observation):\n",
        "    self.num_steps += observation.shape[0] * observation.shape[1]\n",
        "    input_to_old_mean = observation - self.running_mean\n",
        "    mean_diff = torch.sum(input_to_old_mean / self.num_steps, dim=(0, 1))\n",
        "    self.running_mean = self.running_mean + mean_diff\n",
        "    input_to_new_mean = observation - self.running_mean\n",
        "    var_diff = torch.sum(input_to_new_mean * input_to_old_mean, dim=(0, 1))\n",
        "    self.running_variance = self.running_variance + var_diff\n",
        "\n",
        "  @torch.jit.export\n",
        "  def normalize(self, observation):\n",
        "    variance = self.running_variance / (self.num_steps + 1.0)\n",
        "    variance = torch.clip(variance, 1e-6, 1e6)\n",
        "    return ((observation - self.running_mean) / variance.sqrt()).clip(-5, 5)\n",
        "\n",
        "  @torch.jit.export\n",
        "  def get_logits_action(self, observation):\n",
        "    observation = self.normalize(observation)\n",
        "    logits = self.policy(observation)\n",
        "    loc, scale = self.dist_create(logits)\n",
        "    action = self.dist_sample_no_postprocess(loc, scale)\n",
        "    return logits, action\n",
        "\n",
        "  @torch.jit.export\n",
        "  def compute_gae(self, truncation, termination, reward, values,\n",
        "                  bootstrap_value):\n",
        "    truncation_mask = 1 - truncation\n",
        "    # Append bootstrapped value to get [v1, ..., v_t+1]\n",
        "    values_t_plus_1 = torch.cat(\n",
        "        [values[1:], torch.unsqueeze(bootstrap_value, 0)], dim=0)\n",
        "    deltas = reward + self.discounting * (\n",
        "        1 - termination) * values_t_plus_1 - values\n",
        "    deltas *= truncation_mask\n",
        "\n",
        "    acc = torch.zeros_like(bootstrap_value)\n",
        "    vs_minus_v_xs = torch.zeros_like(truncation_mask)\n",
        "\n",
        "    for ti in range(truncation_mask.shape[0]):\n",
        "      ti = truncation_mask.shape[0] - ti - 1\n",
        "      acc = deltas[ti] + self.discounting * (\n",
        "          1 - termination[ti]) * truncation_mask[ti] * self.lambda_ * acc\n",
        "      vs_minus_v_xs[ti] = acc\n",
        "\n",
        "    # Add V(x_s) to get v_s.\n",
        "    vs = vs_minus_v_xs + values\n",
        "    vs_t_plus_1 = torch.cat([vs[1:], torch.unsqueeze(bootstrap_value, 0)], 0)\n",
        "    advantages = (reward + self.discounting *\n",
        "                  (1 - termination) * vs_t_plus_1 - values) * truncation_mask\n",
        "    return vs, advantages\n",
        "\n",
        "  @torch.jit.export\n",
        "  def loss(self, td: Dict[str, torch.Tensor]):\n",
        "    observation = self.normalize(td['observation'])\n",
        "    policy_logits = self.policy(observation[:-1])\n",
        "    baseline = self.value(observation)\n",
        "    baseline = torch.squeeze(baseline, dim=-1)\n",
        "\n",
        "    # Use last baseline value (from the value function) to bootstrap.\n",
        "    bootstrap_value = baseline[-1]\n",
        "    baseline = baseline[:-1]\n",
        "    reward = td['reward'] * self.reward_scaling\n",
        "    termination = td['done'] * (1 - td['truncation'])\n",
        "\n",
        "    loc, scale = self.dist_create(td['logits'])\n",
        "    behaviour_action_log_probs = self.dist_log_prob(loc, scale, td['action'])\n",
        "    loc, scale = self.dist_create(policy_logits)\n",
        "    target_action_log_probs = self.dist_log_prob(loc, scale, td['action'])\n",
        "\n",
        "    with torch.no_grad():\n",
        "      vs, advantages = self.compute_gae(\n",
        "          truncation=td['truncation'],\n",
        "          termination=termination,\n",
        "          reward=reward,\n",
        "          values=baseline,\n",
        "          bootstrap_value=bootstrap_value)\n",
        "\n",
        "    rho_s = torch.exp(target_action_log_probs - behaviour_action_log_probs)\n",
        "    surrogate_loss1 = rho_s * advantages\n",
        "    surrogate_loss2 = rho_s.clip(1 - self.epsilon,\n",
        "                                 1 + self.epsilon) * advantages\n",
        "    policy_loss = -torch.mean(torch.minimum(surrogate_loss1, surrogate_loss2))\n",
        "\n",
        "    # Value function loss\n",
        "    v_error = vs - baseline\n",
        "    v_loss = torch.mean(v_error * v_error) * 0.5 * 0.5\n",
        "\n",
        "    # Entropy reward\n",
        "    entropy = torch.mean(self.dist_entropy(loc, scale))\n",
        "    entropy_loss = self.entropy_cost * -entropy\n",
        "\n",
        "    return policy_loss + v_loss + entropy_loss"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWbuk7IAR0SU"
      },
      "source": [
        "Finally, some code for unrolling and batching environment data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3y5o7-oSBm-"
      },
      "source": [
        "StepData = collections.namedtuple(\n",
        "    'StepData',\n",
        "    ('observation', 'logits', 'action', 'reward', 'done', 'truncation'))\n",
        "\n",
        "\n",
        "def sd_map(f: Callable[..., torch.Tensor], *sds) -> StepData:\n",
        "  \"\"\"Map a function over each field in StepData.\"\"\"\n",
        "  items = {}\n",
        "  keys = sds[0]._asdict().keys()\n",
        "  for k in keys:\n",
        "    items[k] = f(*[sd._asdict()[k] for sd in sds])\n",
        "  return StepData(**items)\n",
        "\n",
        "\n",
        "def eval_unroll(agent, env, length):\n",
        "  \"\"\"Return number of episodes and average reward for a single unroll.\"\"\"\n",
        "  observation = env.reset()\n",
        "  episodes = torch.zeros((), device=agent.device)\n",
        "  episode_reward = torch.zeros((), device=agent.device)\n",
        "  for _ in range(length):\n",
        "    _, action = agent.get_logits_action(observation)\n",
        "    observation, reward, done, _ = env.step(Agent.dist_postprocess(action))\n",
        "    episodes += torch.sum(done)\n",
        "    episode_reward += torch.sum(reward)\n",
        "  return episodes, episode_reward / episodes\n",
        "\n",
        "\n",
        "def train_unroll(agent, env, observation, num_unrolls, unroll_length):\n",
        "  \"\"\"Return step data over multple unrolls.\"\"\"\n",
        "  sd = StepData([], [], [], [], [], [])\n",
        "  for _ in range(num_unrolls):\n",
        "    one_unroll = StepData([observation], [], [], [], [], [])\n",
        "    for _ in range(unroll_length):\n",
        "      logits, action = agent.get_logits_action(observation)\n",
        "      observation, reward, done, info = env.step(Agent.dist_postprocess(action))\n",
        "      one_unroll.observation.append(observation)\n",
        "      one_unroll.logits.append(logits)\n",
        "      one_unroll.action.append(action)\n",
        "      one_unroll.reward.append(reward)\n",
        "      one_unroll.done.append(done)\n",
        "      one_unroll.truncation.append(info['truncation'])\n",
        "    one_unroll = sd_map(torch.stack, one_unroll)\n",
        "    sd = sd_map(lambda x, y: x + [y], sd, one_unroll)\n",
        "  td = sd_map(torch.stack, sd)\n",
        "  return observation, td\n",
        "\n",
        "\n",
        "def train(\n",
        "    env_name: str = 'ant',\n",
        "    num_envs: int = 2048,\n",
        "    episode_length: int = 1000,\n",
        "    device: str = 'cuda',\n",
        "    num_timesteps: int = 30_000_000,\n",
        "    eval_frequency: int = 10,\n",
        "    unroll_length: int = 5,\n",
        "    batch_size: int = 1024,\n",
        "    num_minibatches: int = 32,\n",
        "    num_update_epochs: int = 4,\n",
        "    reward_scaling: float = .1,\n",
        "    entropy_cost: float = 1e-2,\n",
        "    discounting: float = .97,\n",
        "    learning_rate: float = 3e-4,\n",
        "    progress_fn: Optional[Callable[[int, Dict[str, Any]], None]] = None,\n",
        "):\n",
        "  \"\"\"Trains a policy via PPO.\"\"\"\n",
        "  gym_name = f'brax-{env_name}-v0'\n",
        "  if gym_name not in gym.envs.registry.env_specs:\n",
        "    entry_point = functools.partial(envs.create_gym_env, env_name=env_name)\n",
        "    gym.register(gym_name, entry_point=entry_point)\n",
        "  env = gym.make(gym_name, batch_size=num_envs, episode_length=episode_length)\n",
        "  # automatically convert between jax ndarrays and torch tensors:\n",
        "  env = to_torch.JaxToTorchWrapper(env, device=device)\n",
        "\n",
        "  # env warmup\n",
        "  env.reset()\n",
        "  action = torch.zeros(env.action_space.shape).to(device)\n",
        "  env.step(action)\n",
        "\n",
        "  # create the agent\n",
        "  policy_layers = [\n",
        "      env.observation_space.shape[-1], 64, 64, env.action_space.shape[-1] * 2\n",
        "  ]\n",
        "  value_layers = [env.observation_space.shape[-1], 64, 64, 1]\n",
        "  agent = Agent(policy_layers, value_layers, entropy_cost, discounting,\n",
        "                reward_scaling, device)\n",
        "  agent = torch.jit.script(agent.to(device))\n",
        "  optimizer = optim.Adam(agent.parameters(), lr=learning_rate)\n",
        "\n",
        "  sps = 0\n",
        "  total_steps = 0\n",
        "  total_loss = 0\n",
        "  for eval_i in range(eval_frequency + 1):\n",
        "    if progress_fn:\n",
        "      t = time.time()\n",
        "      with torch.no_grad():\n",
        "        episode_count, episode_reward = eval_unroll(agent, env, episode_length)\n",
        "      duration = time.time() - t\n",
        "      # TODO: only count stats from completed episodes\n",
        "      episode_avg_length = env.num_envs * episode_length / episode_count\n",
        "      eval_sps = env.num_envs * episode_length / duration\n",
        "      progress = {\n",
        "          'eval/episode_reward': episode_reward,\n",
        "          'eval/completed_episodes': episode_count,\n",
        "          'eval/avg_episode_length': episode_avg_length,\n",
        "          'speed/sps': sps,\n",
        "          'speed/eval_sps': eval_sps,\n",
        "          'losses/total_loss': total_loss,\n",
        "      }\n",
        "      progress_fn(total_steps, progress)\n",
        "\n",
        "    if eval_i == eval_frequency:\n",
        "      break\n",
        "\n",
        "    observation = env.reset()\n",
        "    num_steps = batch_size * num_minibatches * unroll_length\n",
        "    num_epochs = num_timesteps // (num_steps * eval_frequency)\n",
        "    num_unrolls = batch_size * num_minibatches // env.num_envs\n",
        "    total_loss = 0\n",
        "    t = time.time()\n",
        "    for _ in range(num_epochs):\n",
        "      observation, td = train_unroll(agent, env, observation, num_unrolls,\n",
        "                                     unroll_length)\n",
        "\n",
        "      # make unroll first\n",
        "      def unroll_first(data):\n",
        "        data = data.swapaxes(0, 1)\n",
        "        return data.reshape([data.shape[0], -1] + list(data.shape[3:]))\n",
        "      td = sd_map(unroll_first, td)\n",
        "\n",
        "      # update normalization statistics\n",
        "      agent.update_normalization(td.observation)\n",
        "\n",
        "      for _ in range(num_update_epochs):\n",
        "        # shuffle and batch the data\n",
        "        with torch.no_grad():\n",
        "          permutation = torch.randperm(td.observation.shape[1], device=device)\n",
        "          def shuffle_batch(data):\n",
        "            data = data[:, permutation]\n",
        "            data = data.reshape([data.shape[0], num_minibatches, -1] +\n",
        "                                list(data.shape[2:]))\n",
        "            return data.swapaxes(0, 1)\n",
        "          epoch_td = sd_map(shuffle_batch, td)\n",
        "\n",
        "        for minibatch_i in range(num_minibatches):\n",
        "          td_minibatch = sd_map(lambda d: d[minibatch_i], epoch_td)\n",
        "          loss = agent.loss(td_minibatch._asdict())\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          total_loss += loss\n",
        "\n",
        "    duration = time.time() - t\n",
        "    total_steps += num_epochs * num_steps\n",
        "    total_loss = total_loss / (num_epochs * num_update_epochs * num_minibatches)\n",
        "    sps = num_epochs * num_steps / duration"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2A9MMlHUajH"
      },
      "source": [
        "Let's go!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-lrKHvkUeYM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "fac89efb-ec09-4cb9-ecd7-2f4e954bc920"
      },
      "source": [
        "# temporary fix to cuda memory OOM\n",
        "os.environ['XLA_PYTHON_CLIENT_ALLOCATOR'] = 'platform'\n",
        "\n",
        "xdata = []\n",
        "ydata = []\n",
        "eval_sps = []\n",
        "train_sps = []\n",
        "times = [datetime.now()]\n",
        "\n",
        "def progress(num_steps, metrics):\n",
        "  times.append(datetime.now())\n",
        "  xdata.append(num_steps)\n",
        "  ydata.append(metrics['eval/episode_reward'])\n",
        "  eval_sps.append(metrics['speed/eval_sps'])\n",
        "  train_sps.append(metrics['speed/sps'])\n",
        "  clear_output(wait=True)\n",
        "  plt.xlim([0, 30_000_000])\n",
        "  plt.ylim([0, 6000])\n",
        "  plt.xlabel('# environment steps')\n",
        "  plt.ylabel('reward per episode')\n",
        "  plt.plot(xdata, ydata)\n",
        "  plt.show()\n",
        "\n",
        "train(progress_fn=progress)\n",
        "\n",
        "print(f'time to jit: {times[1] - times[0]}')\n",
        "print(f'time to train: {times[-1] - times[1]}')\n",
        "print(f'eval steps/sec: {np.mean(eval_sps)}')\n",
        "print(f'train steps/sec: {np.mean(train_sps)}')\n",
        "!nvidia-smi -L"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time to jit: 0:03:23.786113\n",
            "time to train: 0:03:42.913398\n",
            "eval steps/sec: 480208.42251490447\n",
            "train steps/sec: 150517.30309979225\n",
            "GPU 0: Tesla V100-SXM2-16GB (UUID: GPU-8bc7bf18-8df5-a5b8-1499-097e5c48c3db)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2p-20bCi4iI"
      },
      "source": [
        "In this arrangement, we can rollout environment steps much faster than we can train: the speed at which PyTorch can backpropagate the loss and step the optimizer is the bottleneck.  This PyTorch code can probably be sped up by adding [automatic mixed precision](https://pytorch.org/docs/stable/notes/amp_examples.html), and following other recommendations in the [PyTorch performance tuning guide](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html).\n",
        "\n",
        "We know we have a fair bit of headroom to improve the PyTorch implementation, as the built-in Brax trainer (which uses [flax.optim](https://flax.readthedocs.io/en/latest/flax.optim.html)) runs at more than double the steps per second:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xmuz3I21p35H",
        "outputId": "0b9cc386-f890-4ff7-abc9-6bae663a9e44"
      },
      "source": [
        "train_sps = []\n",
        "\n",
        "def progress(_, metrics):\n",
        "  train_sps.append(metrics['speed/sps'])\n",
        "\n",
        "ppo.train(\n",
        "    environment_fn=envs.create_fn(env_name='ant'), num_timesteps = 30_000_000,\n",
        "    log_frequency = 10, reward_scaling = .1, episode_length = 1000,\n",
        "    normalize_observations = True, action_repeat = 1, unroll_length = 5,\n",
        "    num_minibatches = 32, num_update_epochs = 4, discounting = 0.97,\n",
        "    learning_rate = 3e-4, entropy_cost = 1e-2, num_envs = 2048,\n",
        "    batch_size = 1024, progress_fn = progress)\n",
        "\n",
        "print(f'train steps/sec: {np.mean(train_sps[1:])}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train steps/sec: 325783.03125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqXKdDwVL6L4"
      },
      "source": [
        "tunaalabagana! ðŸ‘‹"
      ]
    }
  ]
}